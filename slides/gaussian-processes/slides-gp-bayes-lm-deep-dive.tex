\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

% Figure figure/bayes_lm/posterior_5_3.pdf was not found, 
% Changed to prevent compilation error
\newcommand{\titlefigure}{figure/bayes_lm/posterior_5_2.pdf}
\newcommand{\learninggoals}{
\item Know the proof the posterior of bayesian linear model.
\item Know how to derive the predictive distribution of bayesian linear model.
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{The Bayesian Linear Model: Deep Dive}
\lecture{Advanced Machine Learning}

\begin{vbframe}{Proof of the posterior of baysian LM}
\begin{footnotesize}
\textbf{Proof:}\\
    We want to show that 
    \begin{itemize}
      \item for a Gaussian prior on $\thetav \sim \mathcal{N}(\zero, \tau^2 \id_p)$
      \item for a Gaussian Likelihood $y ~|~ \Xmat, \thetav \sim \mathcal{N}(\Xmat^\top \thetav, \sigma^2 \id_n)$ 
    \end{itemize}
    the resulting posterior is Gaussian $\mathcal{N}(\sigma^{-2}\bm{A}^{-1}\Xmat^\top\yv, \bm{A}^{-1})$ with $\bm{A}:= \sigma^{-2}\Xmat^\top\Xmat + \frac{1}{\tau^2} \id_p$.
    
    Plugging in Bayes' rule and multiplying out yields
    \begin{eqnarray*}
    p(\thetav | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetav) q(\thetav) \propto \exp\biggl[-\frac{1}{2\sigma^2}(\yv - \Xmat\thetav)^\top(\yv - \Xmat\thetav)-\frac{1}{2\tau^2}\thetav^\top\thetav\biggr] \\
    &=& \exp\biggl[-\frac{1}{2}\biggl(\underbrace{\sigma^{-2}\yv^\top\yv}_{\text{doesn't depend on } \thetav} - 2 \sigma^{-2} \yv^\top \Xmat \thetav + \sigma^{-2}\thetav^\top \Xmat^\top \Xmat \thetav  + \tau^{-2} \thetav^\top\thetav \biggr)\biggr] \\
    &\propto& \exp\biggl[-\frac{1}{2}\biggl(\sigma^{-2}\thetav^\top \Xmat^\top \Xmat \thetav  + \tau^{-2} \thetav^\top\thetav  - 2 \sigma^{-2} \yv^\top \Xmat \thetav \biggr)\biggr] \\
    &=& \exp\biggl[-\frac{1}{2}\thetav^\top\underbrace{\biggl(\sigma^{-2} \Xmat^\top \Xmat + \tau^{-2} \id_p \biggr)}_{:= \Amat} \thetav + \textcolor{red}{\sigma^{-2} \yv^\top \Xmat \thetav}\biggr]
    \end{eqnarray*}
    
    This expression resembles a normal density - except for the term in red!
    
    \framebreak

    \textbf{Note:} We need not worry about the normalizing constant since its mere role is to convert probability functions to density functions with a total probability of one.
    
    
    We subtract a (not yet defined) constant $c$ while compensating for this change by adding the respective terms (\enquote{adding $0$}), emphasized in green:
    
    \begin{eqnarray*}
    	p(\thetav | \Xmat, \yv) &\propto&  \exp\biggl[-\frac{1}{2}(\thetav \textcolor{green}{- c})^\top\Amat  (\thetav \textcolor{green}{- c}) \textcolor{green}{- c^\top \Amat \thetav} + \underbrace{\textcolor{green}{\frac{1}{2}c^\top\Amat c}}_{\text{doesn't depend on } \thetav} +\sigma^{-2} \yv^\top \Xmat \thetav\biggr] \\
    	&\propto& \exp\biggl[-\frac{1}{2}(\thetav \textcolor{green}{- c})^\top\Amat  (\thetav \textcolor{green}{- c}) \textcolor{green}{- c^\top \Amat \thetav} +\sigma^{-2} \yv^\top \Xmat \thetav\biggr]
    \end{eqnarray*}
    
    If we choose $c$ such that $- c^\top \Amat \thetav +\sigma^{-2} \yv^\top \Xmat \thetav = 0$, the posterior is normal with mean $c$ and covariance matrix $\Amat^{-1}$. Taking into account that $\Amat$ is symmetric, this is if we choose
    
    \begin{eqnarray*}
    && \sigma^{-2} \yv^\top \Xmat = c^\top\Amat \\
    &\Leftrightarrow & \sigma^{-2} \yv^\top \Xmat \Amat^{-1} = c^\top \\
    &\Leftrightarrow& c = \sigma^{-2} \Amat^{-1} \Xmat^\top \yv
    \end{eqnarray*}
    
    as claimed.
    
    \end{footnotesize}
\end{vbframe}

\begin{vbframe}{Predictive Distribution}
    Based on the posterior distribution 
    
    $$
    \thetav ~|~ \Xmat, \yv \sim \mathcal{N}(\sigma^{-2}\bm{A}^{-1}\Xmat^\top\yv, \bm{A}^{-1})
    $$
    
    we can derive the predictive distribution for a new observations $\xv_*$. The predictive distribution for the Bayesian linear model, i.e. the distribution of $\thetav^\top \xv_*$, is 
    
    $$
    y_* ~|~ \Xmat, \yv, \xv_* \sim \mathcal{N}(\sigma^{-2}\yv^\top \Xmat \Amat^{-1}\xv_*, \xv_*^\top\Amat^{-1}\xv_*)
    $$

    Note that $y_* = \thetav^T \xv_* + \epsilon$, where both the posterior of $\thetav$ and $\epsilon$ are Gaussians. By applying the rules for linear transformations of Gaussians, we can confirm that $y_* ~|~ \Xmat, \yv, \xv_*$ is a Gaussian, too. 
\end{vbframe}

\end{document}
