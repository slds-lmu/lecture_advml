\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}


\newcommand{\titlefigure}{figure_man/discrete/marginalization-more.png} %not best picture
\newcommand{\learninggoals}{
  \item GPs model distributions over functions 
  \item The marginalization property makes this distribution easily tractable
 % \item GPs are fully specified by mean and covariance function 
 % \item GPs are indexed families
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gaussian Processes: From Discrete to Continuous}
\lecture{Advanced Machine Learning}

\begin{vbframe}{From Discrete to Continuous Functions}

\begin{itemize}
  \item We defined distributions on functions with finite domain by putting a finite Gaussian on it
  $$
    \mathbf{f} = [f(\xi[1]), f(\xi[2]), \dots, f(\xi[n])] \sim \mathcal{N}(\bm{m}, \bm{K})
  $$

  \item We can do this for $n \to \infty$ (as \enquote{granular} as we want)
  \begin{figure}
    \includegraphics[width = 0.9\textwidth]{figure/discrete/example_limit.pdf}
  \end{figure}
\end{itemize}

\end{vbframe}

\begin{frame}{From Discrete to Continuous Functions}


\begin{itemize}
  \item No matter how large $n$ is, we are still considering a function over a discrete domain. 
  \item How can we extend our definition to functions with \textbf{continuous domain} $\Xspace \subset \R$?
  \item Intuitively, a function $f$ drawn from \textbf{Gaussian process} can be understood as an \enquote{infinite} long Gaussian random vector. 
  \item It is unclear how to handle an \enquote{infinite} long Gaussian random vector!
\end{itemize}

\end{frame}


\begin{frame}{Gaussian Processes: Intuition}

  \begin{itemize}
  \only<1-3>{
    \item Thus, it is required that for \textbf{any finite set} of inputs $\{\xi[1], \dots, \xi[n]\} \subset \Xspace$, the vector $\mathbf{f}$ has a Gaussian distribution
    $$
      \bm{f} = \left[f\left(\xi[1]\right), \dots, f\left(\xi[n]\right)\right] \sim \mathcal{N}\left(\bm{m}, \bm{K}\right),
    $$ 
    with $\bm{m}$ and $\bm{K}$ being calculated by a mean function $m(.)$ / covariance function $k(.,.)$.
    \item This property is called \textbf{marginalization property}. 
    \begin{figure}
      \only<1>{\includegraphics[width=0.4\textwidth]{figure/discrete/example_marginalization_5.pdf}\includegraphics[width=0.5\textwidth]{figure_man/discrete/marginalization-5.png}}
      \only<2>{\includegraphics[width=0.4\textwidth]{figure/discrete/example_marginalization_10.pdf}\includegraphics[width=0.5\textwidth]{figure_man/discrete/marginalization-more.png}}
      \only<3>{\includegraphics[width=0.4\textwidth]{figure/discrete/example_marginalization_50.pdf}\includegraphics[width=0.5\textwidth]{figure_man/discrete/marginalization-more.png}}
   \end{figure}
    }
\end{itemize}

\end{frame}


\begin{vbframe}{Gaussian Processes}

This intuitive explanation is formally defined as follows: 

\lz 

A function $\fx$ is generated by a GP $\gp$ if for \textbf{any finite} set of inputs $\left\{\xv^{(1)}, \dots, \xv^{(n)}\right\}$, the associated vector of function values $\bm{f} = \left(f(\xv^{(1)}), \dots, f(\xv^{(n)})\right)$ has a Gaussian distribution

$$
\bm{f} = \left[f\left(\xi[1]\right),\dots, f\left(\xi[n]\right)\right] \sim \mathcal{N}\left(\bm{m}, \bm{K}\right),
$$

with 


\begin{eqnarray*}
\textbf{m} &:=& \left(m\left(\xi\right)\right)_{i}, \quad
\textbf{K} := \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}, 
\end{eqnarray*}
 
where $m(\xv)$ is called mean function and $k(\xv, \xv^\prime)$ is called covariance function. 


\framebreak 

\vspace*{0.5cm} 

A GP is thus \textbf{completely specified} by its mean and covariance function

\vspace*{-0.2cm}
\begin{eqnarray*}
m(\xv) &=& \E[f(\xv)] \\
k(\xv, \xv^\prime) &=& \E\biggl[\left( f(\xv) - \E[f(\xv)] \right) \left( f(\xv^\prime) - \E[f(\xv^\prime)] \right)\biggr]
\end{eqnarray*}

\vfill

\textbf{Note}: For now, we assume $m(\xv) \equiv 0$. This is not necessarily a drastic limitation - thus it is common to consider GPs with a zero mean function. 

% \framebreak

% \vspace*{0.5cm}

% Intuitively, one can think of a function $f$ drawn from a Gaussian process prior as a Gaussian distribution with an \enquote{infinitely} long mean vector and an \enquote{infinite by infinite} covariance matrix.

% \lz

% Each dimension of the Gaussian corresponds to an element $\xv$ from the domain $\mathcal{X}$. The corresponding component of the random vector represents the value of $f(\xv)$.

% \lz

% The \textbf{marginalization property} makes it possible to handle this \enquote{infinite} representation: evaluations of the process on any finite number of points follow a multivariate normal distribution.

\end{vbframe}

\begin{vbframe}{Sampling from a Gaussian process Prior}

We can draw functions from a Gaussian process prior. Let us consider $\fx \sim \mathcal{GP}\left(0, k(\xv, \xv^\prime)\right)$ with the squared exponential covariance function $^{(*)}$

$$
k(\xv, \xv^\prime) = \exp\left(-\frac{1}{2\ls^2}\|\xv - \xv^\prime\|^2\right), ~~ \ls = 1.
$$
\vspace{-4cm}
This specifies the Gaussian process completely. 

\vspace{8cm}
\footnotesize
$^{(*)}$ We will talk later about different choices of covariance functions. 

\normalsize

\framebreak 

To visualize a sample function, we 

\begin{itemize}
  \item choose a high number $n$ (equidistant) points $\left\{\xv^{(1)}, \dots, \xv^{(n)}\right\}$
  \item compute the corresponding covariance matrix $\Kmat = \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}$ by plugging in all pairs $\xv^{(i)}, \xv^{(j)}$ 
  \item sample from a Gaussian $\bm{f} \sim \mathcal{N}(\bm{0}, \bm{K})$. 
\end{itemize}

We draw $10$ times from the Gaussian, to get $10$ different samples.  

% Using $100$ equidistant points, we repeat the process of generating the Gaussian $10$ times ($10$ different functions) and draw each function by connecting the sampled values. 

% \lz

\begin{figure}
  \includegraphics[width=0.9\textwidth]{figure/gp_sample/different_samples.pdf}
\end{figure}

\vspace{-0.2cm}
Since we specified the mean function to be zero $m(\xv) \equiv 0$, the drawn functions have zero mean.

\end{vbframe}


\section{Gaussian Processes as Indexed Family}




\begin{vbframe}{Gaussian processes as an Indexed Family}

% \begin{block}{Definition}
% A \textbf{Gaussian process} is a (infinite) collection of random variables, any \textbf{finite} number of which have a \textbf{joint Gaussian distribution}.
% \end{block}

% \lz

A Gaussian process is a special case of a \textbf{stochastic process} which is defined as a collection of random variables indexed by some index set (also called an \textbf{indexed family}). What does it mean? 

\lz 

An \textbf{indexed family} is a mathematical function (or \enquote{rule}) to map indices $t \in T$ to objects in $\mathcal{S}$. 

\begin{block}{Definition}
A \textbf{family of elements in $\mathcal{S}$ indexed by $T$} (indexed family) is a surjective function 
\vspace*{-0.3cm}
\begin{eqnarray*}
s: T &\to& \mathcal{S} \\
   t &\mapsto& s_t = s(t) 
\end{eqnarray*}
\end{block}

\end{vbframe}

\begin{vbframe}{Indexed Family}

Some simple examples for indexed families are:

\vspace*{0.3cm}

\begin{minipage}{0.43\linewidth}
  \begin{itemize}
  \item finite sequences (lists): $T = \{1, 2, \dots, n\}$ and $\left(s_t\right)_{t \in T} \in \R$ \vspace{1cm}
  \item infinite sequences: $T = \N$ and $\left(s_t\right)_{t \in T} \in \R$
  \end{itemize}
\end{minipage}
\begin{minipage}{0.55\linewidth}
\includegraphics{figure_man/indexed_family/indexed_family_1.png} \\
\includegraphics{figure_man/indexed_family/indexed_family_2.png}
\end{minipage}


\framebreak

But the indexed set $\mathcal{S}$ can be something more complicated, for example functions or \textbf{random variables} (RV):

\begin{minipage}{0.43\linewidth}
  \vspace*{0.5cm}
  \begin{itemize}
    \item $T = \{1, \dots, m\}$, $Y_t$'s are RVs: Indexed family is a random vector. \vspace*{0.2cm}
    \item $T = \{1, \dots, m\}$, $Y_t$'s are RVs: Indexed family is a stochastic process in discrete time \vspace*{0.2cm}
    \item $T = \Z^2$, $Y_t$'s are RVs: Indexed family is a 2D-random walk.
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}{0.5\linewidth}
\includegraphics{figure_man/indexed_family/indexed_family_4.png} \\
\includegraphics{figure_man/indexed_family/indexed_family_3.png}
\end{minipage}

\end{vbframe}

\begin{frame}{Indexed Family}

\begin{itemize}
  \item A Gaussian process is also an indexed family, where the random variables $f(\xv)$ are indexed by the input values $\xv \in \Xspace$. 
  \item Their special feature: Any indexed (finite) random vector has a multivariate Gaussian distribution (which comes with all the nice properties of Gaussianity!). 
\end{itemize}

\begin{figure}
  \includegraphics<1>[width=0.7\textwidth]{figure_man/indexed_family/indexed_family_5.png} \par
  \only<1>{\begin{footnotesize} Visualization for a one-dimensional $\Xspace$. \end{footnotesize}}
  \includegraphics<2>[width=0.6\textwidth]{figure_man/indexed_family/indexed_family_6.png}\par
  \only<2>{\begin{footnotesize} Visualization for a two-dimensional $\Xspace$. \end{footnotesize}}
\end{figure}

\end{frame}


\endlecture
\end{document}
