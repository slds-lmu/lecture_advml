\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-online}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/batch_vs_online_sec}
\newcommand{\learninggoals}{
  \item Understand the difference between batch and online learning
  \item Know the basic and the extended learning protocol in online learning
  \item Know how performance is measured in online learning
}

%\newcommand{\Tspace}{\mathcal{T}}
%\newcommand{\tv}{\mathbf{t}}
%\newcommand{\tj}{\mathbf{t}_j}
\renewcommand{\l}{L}


\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Introduction to Online Learning}
\lecture{Advanced Machine Learning}



\sloppy

\begin{frame} [t]
	\frametitle{Batch learning}
	%	
	\small
	\begin{itemize} 
		%			
			%	
			  \item The conventional machine learning is rooted in the \emph{statistical learning theory} and is sometimes referred to as the \emph{batch learning scenario}:	
			  \lz
			  
%			
			\begin{minipage}{.6\textwidth}
			\begin{itemize}\small
%				
				  \item A data set $\D = \big\{(\xi,\yi)\big\}_{i=1}^n $   is given beforehand in form of a random sample (iid observations).
%				  
				  \pause
				  \item [$\leadsto$] a \emph{batch} of data
%				  
				  \item The goal is to learn a single predictor (model), i.e., a mapping $f:\Xspace \to \Yspace$ that will have a good prediction accuracy (low risk) on future, unseen data in $\Xspace \times \Yspace$. 
%				  
				\pause
			\end{itemize}
		\end{minipage}
		\begin{minipage}{.3\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=0.99\linewidth]{figure/batch_learning}
			\end{figure}
			%	
		\end{minipage}
		%	
		\lz
		  \item The learning task on the available data beforehand is called the \emph{training phase} and the prediction on the unseen data is called the \emph{testing phase.} Both phases are \textbf{separated}.
		%	
	\end{itemize}
	%    
\end{frame}

\begin{frame}[t]
	\frametitle{Online learning}
%	
\begin{itemize} \small
%	
  	\item However, many real-world problems are \emph{dynamic} with the following aspects:
	%
	\lz
	
	\begin{minipage}{.6\textwidth}
	\begin{itemize} \small
		%	
		\item \emph{Sequential order ---} data is generated only bit by bit;
		%	
		\item \emph{On-the-fly decisions ---} decisions or predictions have to be made during the data generating process;
		%	
		\item \emph{Unforeseeable consequences ---} decisions can have a drastic influence on the data generating process;
		%	
		\item \emph{Constraints ---} there is a specific time limit or computational limit for the decision.
		%
	\end{itemize}
	\end{minipage}
	\begin{minipage}{.3\textwidth}
		\begin{figure}
		\centering
		\includegraphics[width=0.99\linewidth]{figure/online_learning}
		\end{figure}
	%	
	\end{minipage}
	%
	\lz
	{\visible<2-> {	\item These dynamic aspects outline the framework where \textbf{online learning} is settled. 
	%
	\item Characteristically: In the online learning scenario the training phase and the testing phase are \textbf{interleaved}.}}
	%	
	%		
	\end{itemize}
%	
\end{frame}


\begin{frame}
	\frametitle{Online learning: Examples}
	%	
	\small
	There are many real-world applications which fit into the online learning scenario:
	%
	\begin{itemize} \small
		%
		  \item \emph{Weather Forecasting ---} Observe meteorological data as data streams by satellites for instance and keep the current weather prediction up to date.
		%	 
		%
			\pause
		  \item \emph{Sequential Investment ---} A bank has to allocate its total capital on the financial market, where asset prices are varying over time.
		%
		\pause
		%	 
		  \item \emph{Navigation systems ---} Find the best path from A to B given the current traffic situation.
		%
		\pause
		%	 
		  \item \emph{Autonomous driving systems ---} Steer the automotive, while constantly monitoring the environment and react appropriately to any changes.
		%
		  \item $\ldots$
%		  
	\end{itemize}
	%
\end{frame}


\begin{frame}
\frametitle{Online learning: Illustration}
\small
%	
{\visible<1-> The data is available only in a \textbf{sequential order} generated by the environment and the learner's actions/predictions have to be made \textbf{on-the-fly.}}
%

{\visible<2-> {$\Rightarrow $ Learning algorithms have to be dynamically adapted (Update of internal model).\\~} }
 
{\only<1>{
\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{figure/online_step_1}
\end{figure}}}
{\only<2>{
		\begin{figure}
			\centering
			\includegraphics[width=0.4\linewidth]{figure/online_step_1_5}
\end{figure}}}
{\only<3>{
		\begin{figure}
			\centering
			\includegraphics[width=0.4\linewidth]{figure/online_step_2}
\end{figure}}}
{\only<4>{
		\begin{figure}
			\centering
			\includegraphics[width=0.4\linewidth]{figure/online_step_2_5}
\end{figure}}}
{\only<5>{
		\begin{figure}
			\centering
			\includegraphics[width=0.4\linewidth]{figure/online_step_3}
\end{figure}}}


{\only<5>{$\Rightarrow $  The learner and the environment are alternately performing their actions.}}

\end{frame}




\begin{frame}{The basic online learning protocol}
	%	
	\footnotesize
	%	
	Formally, an online learning problem consists of:
	%
	\begin{itemize}\footnotesize
		%
		 \item a learner (forecaster, agent resp.\ decision maker), an environment (user resp.\ adversary, system resp.\ nature),
		%	
		 \item time steps $1,2,\ldots,T$ (may be infinite),
		%	
		 \item available actions $\mathcal A$ for the learner (may be infinite),
		%
		 \item environmental data space $\mathcal Z,$
		%
		 \item a loss function $\l: \mathcal{A} \times \mathcal{Z} \to \R.$
	\end{itemize}
	%
	{\visible<2-> {\textbf{Mechanism:} In each time step $t$
	%
	\begin{itemize}\footnotesize
		%
		 \item learner chooses an action $a_{t} \in \mathcal{A},$
		%
		 \item environment generates data $z_{t} \in \mathcal{Z},$
		%
		 \item learner observes the environmental data and suffers loss $\l(a_{t},z_{t}),$ 
		%	
		 \item learner update its model/ knowledge basis.
		%	
	\end{itemize}}}
	%
	{\visible<3-> {	Typically $\mathcal{A} = \mathcal{Z} = \Yspace$, so that
	%
	%
	\begin{itemize}\footnotesize
		%
		\item the learner's chosen action $a_{t} = \hat{y}_t$ corresponds to a prediction,
		%
		\item the generated data point $z_{t} =y_t$ is the revealed outcome.
		%	
	\end{itemize}}}
\end{frame}

\begin{frame}{The extended online learning protocol}
	%	
\footnotesize
\begin{itemize}
	%		
	 \item In some applications, the environmental data consists of two parts: $z_{t} = (z_{t}^{(1)}, z_{t}^{(2)}),$ where the first part of the data, $z_{t}^{(1)},$ is revealed to the learner \textbf{before} the action is made.
	%
	After the learner carries out its action, the remaining part of the environmental data is revealed, that is, $z_{t}^{(2)}.$
	%
	{\visible<2-> {  \item The \textbf{mechanism} in such an online learning problem is then as follows: In each time step $t$
	%
	\begin{itemize}\footnotesize
		%
		 \item the environment generates data $z_{t} = (z_{t}^{(1)}, z_{t}^{(2)}) \in \mathcal{Z},$
		%	
		 \item the learner observes the first part of the environmental data $ z_{t}^{(1)},$
		%	
		 \item the learner chooses an action $a_{t} \in \mathcal{A},$  
		%
		 \item the learner observes the remaining part of the environmental data $z_{t}^{(2)}$ and suffers loss $\l(a_{t},z_{t}),$ 
		%	
		 \item the learner updates its knowledge base.
		%	
	\end{itemize} }}
	%
	{\visible<3-> {  \item Apparently, the learner can  take the a priori information in form of $ z_{t}^{(1)}$ at each time step $t$ into account when choosing its action. }}
	% 
	%
	{\visible<4-> {  \item We call this setting the \emph{extended online learning protocol.}	}}
%	 
	 {\visible<5-> { \item 	Typically $\mathcal{A} = \Yspace$ and  $ \mathcal{Z} = \Xspace \times \Yspace$, so that
	 		%
	 		%
	 		\begin{itemize}\footnotesize
	 			%
	 			\item the first part $z_{t}^{(1)} = \xv_t$ is some feature information,
	 			%	
	 			\item the learner's chosen action $a_{t} = \hat{y}_t$ corresponds to a prediction (dep.\ on $\xv_t$),
%	 			
				\item the second part $z_{t}^{(2)} =y_t$ is the corresponding outcome.
%	 			
	 \end{itemize}}}	
\end{itemize}
%
\end{frame}


\begin{frame}{Data generation in Online learning}
%	
	\begin{itemize}
%		
		\item Typically for the online learning setting is that \textbf{no} statistical assumptions is made on how the sequence of environmental data is generated.
%		
		\item In particular, the environmental data are not necessarily generated by a probability distribution!
%		
		\pause
		\item This also covers the area of \emph{adversarial learning}: the data can even be generated by an adversary trying to fool the learner.
%		
		\item However, the online learning setting can of course also be considered in a statistical setting.
%		
	\end{itemize}
%	
\end{frame}


\begin{frame}{Online learning: Requirements}
	%
	\small
	\begin{itemize}
		%	
		\item The dynamical aspects have to be incorporated for the design of efficient learning algorithms. 
		%	
		 \item The online learner has to cope with the sequential availability of the data and to cope with time as well as computational constraints.
		%	
		 \item Roughly speaking, one seeks to construct an online learning algorithm which is adaptive to the environment and allows incremental as well as preferably cheap updates over time.
		%	
		\pause 
		 \item Although consideration of time and memory constraints is important for practical purposes, we will only implicitly consider these constraints in this lecture. 
		%	
		 \item We will mainly focus our theoretical analysis on the performance of the learner in terms of its (cumulative) loss, which, however, will usually ignore computational aspects of the learner.
		%
	\end{itemize}
	%
\end{frame}


\begin{frame} 
	{Measure of Quality  in Online learning}
	%
	\small
	%
	\begin{itemize}
		%	
		\item In order to measure the quality of an online learner one can compute the difference between the cumulative loss of the learner and the cumulative loss by taking some competing action $a\in \mathcal{A}:$
		%
		\begin{align*}
			%	 \label{defi_regret_competing}
			%	
			R_T(a) = \sum\nolimits_{t=1}^T \l(a_{t},z_{t}) - \sum\nolimits_{t=1}^T \l(a, z_{t}).
			%	
		\end{align*}
		%
		 \item This value is called the \emph{(cumulative) regret of a learner} with respect to an action $a \in \mathcal{A}.$
		%
		 \item {\visible<2->{  Here, 
		 %		 
		 \begin{itemize}  \small
		 	%		
		 	\item  $\sum_{t=1}^T \l(a_{t},z_{t})$ is the \emph{cumulative loss of the learner},
		 	%	
		 	\item $\sum_{t=1}^T \l(a, z_{t})$ is the cumulative loss of the competing action $a.$ 
	 \end{itemize} }}
		%
	\end{itemize}
	%
\end{frame}


\begin{frame} 
	{Measure of Quality  in Online learning}
	%
	\small
	%
	\begin{itemize}
		%	
		\item  It seems natural to compare the incurred cumulative loss of the learner with the \emph{best action(s) in hindsight}:
				%
				\begin{align*}
					%	 \label{defi_regret}
					%	
					R_T = \sum\nolimits_{t=1}^T \l(a_{t},z_{t}) - \inf_{a\in \mathcal{A}} \sum\nolimits_{t=1}^T \l(a, z_{t}).
					%	
				\end{align*} 
				%
	 {\visible<2->{	\item Here, 
				%		 
				\begin{itemize} \small
					%		
					\item  $\sum_{t=1}^T \l(a_{t},z_{t})$ is the \emph{cumulative loss of the learner},
					%	
					\item $\inf_{a\in \mathcal{A}} \sum_{t=1}^T \l(a, z_{t})$ is the cumulative loss of the \emph{best action(s) in hindsight}. 
		\end{itemize} }}
		%
		\item {\visible<3>{  We refer to $R_T$ as the \emph{(cumulative) regret} of the online learner. It is easy to see that $R_T = \sup_{a\in \mathcal{A}} R_T(a).$ }}
		%
	\end{itemize}
	%
\end{frame}

%	
\begin{frame} 
	{Measure of Quality  in Online learning}
	%
	\small
	\begin{itemize}
		\item The objective of the online learner is to minimize the cumulative regret $R_T.$
		%	
		 \item {\visible<2->{  Note that the cumulative regret can be in principle negative as the action sequence could be such that $\l(a_{s},z_{s}) <    \l(a^*,z_{s})$ holds for specific time steps $s,$ where $a^* \in \argmin_{a \in \mathcal{A} } \sum_{s=1}^T \l(a,z_{s})$ is one of the best actions in hindsight (may be unique). }}
		%	
		%	If this is possible (which will be rarely the case), then the goal of the learner is to achieve the lowest cumulative regret possible.
		%
		 \item {\visible<3->{  If the cumulative regret is always non-negative (which will be usually the case), then the overall goal of an online learner is to have a regret which is \emph{sublinear} in the time horizon $T.$ }}
		%	
		{\visible<4>{  \item Formally, the following should hold 
		%
		\begin{align*}
				R_T	= o(T).	
		\end{align*}
		%
		\emph{Interpretation:} The average regret per time step (or per example) goes to zero: 
		%	
		\begin{align*}
			  \frac1T \Big( \sum_{t=1}^T \l(a_{t},z_{t}) - \inf_{a\in \mathcal{A}} \sum_{t=1}^T \l(a, z_{t}) \Big) = \frac{R_T}{T} = o(1).
		\end{align*} }}
		%
	\end{itemize}
	%
\end{frame}


\begin{frame}
	{Dynamic regret}
%	
	\begin{itemize}
%		
		\item One might ask why one compares only with a fixed best action in hindsight, say $a^*$, instead of a sequence of actions $a_1^*,a_2^*,\ldots,a_T^*?$ 
		%	
		{\visible<2->{  \item The rationale behind this measure of quality is that the best	fixed action in hindsight is already reasonably good over all the time steps: it performs almost as well as a batch learner that
		observes the entire sequence and picks the best action in hindsight. }}
		%	
		{\visible<3->{  \item However, this is too optimistic and may not hold in changing environments, where data are evolving and the optimal action is drifting over the time. }}
		%	
		{\visible<4->{  \item To address this limitation, recent works have also considered the \emph{dynamic regret}:
		%	
		\begin{align*}
			%	 \label{defi_dynamic_regret}
			%	
			R_T^D(a_1^*,a_2^*,\ldots,a_T^*) = \sum\nolimits_{t=1}^T \l(a_{t},z_{t}) -   \sum\nolimits_{t=1}^T \l(a_t^*, z_{t}).
			%	
		\end{align*} }}
%	
		{\visible<5->{  \item We will cover only the static regret in this lecture.}}
%		
	\end{itemize}
%	
\end{frame}



%
\endlecture
\end{document}
