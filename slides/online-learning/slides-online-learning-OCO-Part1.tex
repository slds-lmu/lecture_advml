\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-online}

%\newcommand{\llin}{\l^{\texttt{lin}}}
%\newcommand{\lzeroone}{\l^{0-1}}
%\newcommand{\lhinge}{\l^{\texttt{hinge}}}
%\newcommand{\lexphinge}{\widetilde{\l^{\texttt{hinge}}}}
%\newcommand{\lconv}{\l^{\texttt{conv}}}
%\newcommand{\FTL}{\texttt{FTL}}
%\newcommand{\FTRL}{\texttt{FTRL}}
%\newcommand{\OGD}{{\texttt{OGD}}}
%\newcommand{\EWA}{{\texttt{EWA}}} 
%\newcommand{\REWA}{{\texttt{REWA}}} 
%\newcommand{\EXPthree}{{\texttt{EXP3}}}
%\newcommand{\EXPthreep}{{\texttt{EXP3P}}}
%\newcommand{\reg}{\psi}
%\newcommand{\Algo}{\texttt{Algo}}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/gradienten_verfahren}
\newcommand{\learninggoals}{
  \item Get to know the class of online convex optimization problems
  \item Derive the online gradient descent as a suitable learning algorithm for such cases
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Online Convex Optimization - Part 1}
\lecture{Advanced Machine Learning}



\sloppy

\begin{frame} 
	\frametitle{Online Convex Optimization}
	%	
	\small
	\begin{itemize}
%		
		\item 	One of the most relevant instantiations of the online learning problem is the problem of \emph{online convex optimization} (OCO), which is characterized by a loss function $$\l:\Aspace \times \Zspace \to \R,$$ which is convex w.r.t.\ the action, i.e., 
		$a \mapsto \l(a,z)$ is convex for any $z \in \Zspace.$ 
%		
	 	 {\visible<2->{ \item Note that both OLO and OQO belong to the class of online convex optimization problems:
		%	
		\begin{itemize} \small
			%		
			%		
			\item \emph{Online linear optimization (OLO) with convex action spaces:}  $$\l(a,z)=a^\top z$$ is a convex function in $a\in \Aspace,$ provided $\Aspace$ is convex.
			%		
			\pause
			\item \emph{Online quadratic optimization (OQO) with convex action spaces:}  $$\l(a,z)=\frac12 \norm{a-z}^2$$ is a convex function in $a\in \Aspace,$ provided $\Aspace$ is convex.
			%	{\tiny (We will show the convexity for specific choices in an exercise on the third assignment sheet.)}
			%
		\end{itemize} }}
		%		
	\end{itemize}  
	%	
\end{frame}



\begin{vbframe} 
	\frametitle{Online Gradient Descent: Motivation}
	%	
	\small
	\begin{itemize}
		%		
		\item We have seen that the FTRL algorithm with the $\l_2$ norm regularization $\reg(a) = \frac{1}{2 \eta}  \norm{a}^2$ achieves satisfactory results for online linear optimization (OLO) problems, that is, if $\l(a,z)=\llin(a,z):=a^\top z,$ then we have
		%		
		\begin{itemize} \small
			%			
			\item \emph{Fast updates ---} If $\Aspace = \R^d,$ then 
			$$	a_{t+1}^{\FTRL} = a_t^{\FTRL} - \eta \, z_t, \qquad t=1,\ldots,T; $$ 
			%			
			\item \emph{Regret bounds ---} By an appropriate choice of $\eta$ and some (mild) assumptions on $\Aspace$ and $\Zspace,$ we have $$	R_T^{\FTRL} = o(T).$$
			%			
		\end{itemize}
		%	
	\end{itemize}
%
\end{vbframe}
%
	\begin{frame} 
			\frametitle{Online Gradient Descent: Motivation}
			%	
		\small 
		Apparently, the nice form of the loss function $\llin$ is responsible for the appealing properties of FTRL in this case. 
		Indeed, 
		since $\nabla_a \llin(a,z) = z$   note that the update rule can be written as 
		%		
		\begin{equation*}
%			\label{eq:forel_update}
			%	
			a_{t+1}^{\FTRL} = a_t^{\FTRL} - \eta \, z_t =  a_t^{\FTRL} - \eta \, \nabla_a \llin(a_t^{\FTRL},z_t).
			%	
		\end{equation*}
		%		
		%since $\nabla \llin(a,z) = z.$  
		%		
		\begin{minipage}{.6\textwidth}
		{\visible<2->{ \emph{Interpretation}: In each time step $t+1,$  we are following the direction with the steepest decrease of the most recent loss (represented by $- \nabla \llin(a_t^{\FTRL},z_t)$) from the current ''position'' $a_t^{\FTRL}$ with the step size $\eta$ }}
		\lz 
		
		{\visible<3>{ $\Rightarrow $ Gradient Descent. }}
		
		\end{minipage}
		\begin{minipage}{.3\textwidth}{\visible<3>{ 
			\begin{figure}
				\centering
				\includegraphics[width=0.8\linewidth]{figure/gradienten_verfahren}
			\end{figure}}}
		\end{minipage}
		%		
	%
\end{frame}


\begin{frame} 
	\frametitle{Online Gradient Descent: Motivation}
	%	
	\small
	\begin{itemize}
		%	
		\item \textbf{Question:} How to transfer this idea of the Gradient Descent for the update formula to other loss functions, while still preserving the regret bounds?
		%	
		{\visible<2->{  \item \textbf{Solution (for convex losses):} Recall the equivalent characterization of convexity of differentiable convex functions:
		%	
		\begin{align*}
			%		
			f:S\to \R \mbox{ is convex } 
			&\Leftrightarrow  f(y) \geq f(x)+(y-x)^\top  \nabla f(x)  \mbox{ for any } x,y\in S \\
			 &\Leftrightarrow  f(x) - f(y) \leq (x-y)^\top  \nabla f(x)  \mbox{ for any } x,y\in S.	
			%		
		\end{align*}}
		}
		%		
		{\visible<3>{  \item This means if we are dealing with a loss function $\l:\Aspace \times \Zspace \to \R,$ which is convex and differentiable in its first argument ($\Aspace$ has also to be convex), then 
		%	
		$$		\l(a,z) - \l(\tilde a, z) \leq 	(a-\tilde a )^\top \, \nabla_a \l(a,z), \quad \forall a,\tilde a\in \Aspace, z\in \Zspace. $$}}
		%
	\end{itemize}
	%
\end{frame}


\begin{frame} 
	\frametitle{Online Gradient Descent: Motivation}
	%	
	\footnotesize
	\begin{itemize}
		%
		\item []
		\fbox{\begin{minipage}{0.9\textwidth}
				\textbf{Reminder:} \quad  $		\l(a,z) - \l(\tilde a, z) \leq 	(a-\tilde a )^\top \, \nabla_a \l(a,z), \quad \forall a,\tilde a\in \Aspace, z\in \Zspace. $
			\end{minipage}
		}
		\item {\visible<2->{  Let  $z_1,\ldots,z_T$ arbitrary environmental data and $a_1,\ldots,a_T$ be some arbitrary action sequence. Substitute $\tilde z_t := \nabla_a \l(a_t,z_t)$ and note that }}
		%	
		%
		{\visible<3->{		
		\begin{align*}
			%		
			 	{\color{blue} R_T(\tilde a) } 
					%		 
					&= \sum_{t=1}^T \l(a_t,z_t) -  \l(\tilde a ,z_t) 
			%		
			   \leq  \sum_{t=1}^T  (a_t -\tilde a )^\top \, \nabla_a \l(a_t,z_t)	\\
			%		
			  &= \sum_{t=1}^T  (a_t -\tilde a )^\top \, \tilde z_t 
			%			
			  = \sum_{t=1}^T  a_t^\top \, \tilde z_t  - \tilde a^\top \, \tilde z_t 
			%			
			  =  {\color{orange} \sum_{t=1}^T \llin(a_t,\tilde z_t) - \llin(\tilde a,\tilde z_t)}. 
			%		
		\end{align*}  }}
		%
		\item[] {\visible<4->{ \noindent \emph{Conclusion:} {\color{blue}The regret of a learner with respect to a differentiable and convex loss function $\l$} is bounded by {\color{orange} the regret corresponding to an online linear optimization problem with environmental data $\tilde z_t = \nabla_a \l(a_t,z_t).$} }}
		%	
		\item {\visible<5->{ \textbf{We know:} Online linear optimization problems can be tackled by means of the FTRL algorithm!}}
		%		 
		\item [$\leadsto$] {\visible<6->{ Incorporate the substitution $\tilde z_t = \nabla_a \l(a_t,z_t)$ into the update formula of FTRL with squared L2-norm regularization.}}
		%	
		%
	\end{itemize}
\end{frame}
%
\begin{frame} 
	\frametitle{Online Gradient Descent: Definition}
	\small
	\begin{itemize}
		%	
		\item 
		%	
		The corresponding algorithm which chooses its action according to these considerations is called the \emph{Online Gradient Descent} (OGD) algorithm with step size $\eta>0.$ It holds in particular,
		%	
		%	More specifically, the action are chosen as 
		%	
		\begin{equation}
			\label{eq:OGD_update}
			%		
			a_{t+1}^\OGD = a_{t}^\OGD - \eta \nabla_a \l(a_{t}^\OGD,z_t ), \quad t=1,\ldots T.
			%		
		\end{equation}
		{\tiny (Technical side note:  For this update formula we assume that $\Aspace=\R^d.$ Moreover, the first action $a_1^\OGD$ is arbitrary. )}
		%

		%
	\end{itemize}
\end{frame}
%

%
\endlecture
\end{document}
