\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-online}

%\newcommand{\llin}{\l^{\texttt{lin}}}
%\newcommand{\lzeroone}{\l^{0-1}}
%\newcommand{\lhinge}{\l^{\texttt{hinge}}}
%\newcommand{\lexphinge}{\widetilde{\l^{\texttt{hinge}}}}
%\newcommand{\lconv}{\l^{\texttt{conv}}}
%\newcommand{\FTL}{\texttt{FTL}}
%\newcommand{\FTRL}{\texttt{FTRL}}
%\newcommand{\OGD}{{\texttt{OGD}}}
%\newcommand{\EWA}{{\texttt{EWA}}} 
%\newcommand{\REWA}{{\texttt{REWA}}} 
%\newcommand{\EXPthree}{{\texttt{EXP3}}}
%\newcommand{\EXPthreep}{{\texttt{EXP3P}}}
%\newcommand{\reg}{\psi}
%\newcommand{\Algo}{\texttt{Algo}}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/gradienten_verfahren}
\newcommand{\learninggoals}{
  \item Know the connection between OGD and FTRL via linearization of convex functions
  \item See how this implies regret bounds for OGD
  \item Get to know the theoretical limits for online convex optimization
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Online Convex Optimization - Part 2 -}
\lecture{Advanced Machine Learning}



\sloppy


\begin{frame} 
	\frametitle{Online Gradient Descent}
	\small
	\begin{itemize}
		%	
		\item 
		%	
		The \emph{Online Gradient Descent} (OGD) algorithm with step size $\eta>0$ chooses its action by
		%	
		%	
		\begin{equation}
			\label{eq:OGD_update}
			%		
			a_{t+1}^\OGD = a_{t}^\OGD - \eta \nabla_a \l(a_{t}^\OGD,z_t ), \quad t=1,\ldots T.
			%		
		\end{equation}
		{\tiny (Technical side note:  For this update formula we assume that $\Aspace=\R^d.$ Moreover, the first action $a_1^\OGD$ is arbitrary. )}
		%
		 {\visible<2->{ \item We have the following connection between FTRL and OGD:
		\begin{itemize}\small
			%		
			\item Let $\tilde z_t^\OGD :=  \nabla_a \l(a_{t}^\OGD,z_t )$ for any $t=1,\ldots,T.$ 
			%		be the substituted environmental data.
			%		
			\item The update formula for FTRL with $\l_2$ norm regularization for the linear loss $\llin$ and the environmental data $\tilde z_t^\OGD$ is
			{\footnotesize		$$a_{t+1}^{\FTRL} = a_{t}^{\FTRL} - \eta \tilde z_t^\OGD =  a_{t}^{\FTRL} - \eta  \nabla_a \l(a_{t}^\OGD,z_t ). $$}
			%		
			\item If we have that $a_{1}^{\FTRL} = a_{1}^\OGD,$ then it iteratively follows that $a_{t+1}^{\FTRL} = a_{t+1}^\OGD$ for any $t=1,\ldots,T$ in this case.
			%
		\end{itemize} }}
		%
	\end{itemize}
\end{frame}
%
\begin{frame} 
	\frametitle{Online Gradient Descent: Definition and properties}
	\footnotesize
	\begin{itemize}
		\small
		%
		\item With the deliberations above we can infer that
		%	
		\begin{align*}
%			
		 R_{T,\l}^\OGD(\tilde a ~|~ (z_t)_{t} ) 
%			
			&= \sum\nolimits_{t=1}^T \l(a_t^\OGD,z_t) -  \l(\tilde a ,z_t)  \\
%			
		 	&\leq \sum\nolimits_{t=1}^T \llin(a_t^\OGD,\tilde z_t^\OGD) - \llin(\tilde a,\tilde z_t^\OGD) \\ 
%			
			&\stackrel{\mbox{(if $a_1^\OGD=a_1^{\FTRL}$})}{=} 
			\sum\nolimits_{t=1}^T \llin(a_t^{\FTRL},\tilde z_t^\OGD) - \llin(\tilde a,\tilde z_t^\OGD) \\
			%
			&= R_{T,\llin}^{\FTRL}(\tilde a ~|~ (\tilde z_t^\OGD)_{t}), 
		\end{align*}
		%	
		where we write in the subscripts of the regret the corresponding loss function and also include the corresponding environmental data as a second argument in order to emphasize the connections.
		%	
		\item 
		%	It holds for any $\tilde a \in \Aspace$ that $R_{T,\l}^\OGD(\tilde a| (z_t)_{t})\leq R_{T,\llin}^{\FTRL}(\tilde a| (\tilde z_t)_{t}).$ \\
		{\visible<2->{  \emph{Interpretation:} The regret of the FTRL algorithm (with $\l_2$ norm regularization) for the online linear optimization problem (characterized by the linear loss $\llin$) with environmental data $\tilde z_t^\OGD$ is an upper bound for the OGD algorithm for the online convex problem (characterized by a differentiable convex loss $\l$) with the original environmental data $z_t.$ }}
		%	
	\end{itemize}
\end{frame}
%
\begin{frame} 
	\frametitle{Online Gradient Descent:  Regret}
	\small
	\begin{itemize}
		%	
		%	
		\item Due to this connection we immediately obtain a similar decomposition of the regret upper bound into a bias term and a variance term as for the FTRL algorithm for OLO problems.
		% 
		\item \textbf{Corollary.} Using the OGD algorithm on any online convex optimization problem (with differentiable loss function $\l$)  leads to a regret of OGD with respect to any action $\tilde a \in \Aspace$  of
		%	 
		\begin{align*}
			%	
			R_T^{\OGD}(\tilde a)  
%			
			&\leq \frac{1}{2\eta}  \norm{\tilde a}^2 +   \eta  \sum\nolimits_{t=1}^T \norm{\tilde z_t^\OGD}^2 \\
%			
			&= \frac{1}{2\eta}  \norm{\tilde a}^2 +   \eta  \sum\nolimits_{t=1}^T \norm{ \nabla_a \l(a_t^\OGD,z_t) }^2.
			%		
		\end{align*}	
		%
		 {\visible<2->{ \item Note that the step size $\eta>0$ of OGD has the same role as the regularization magnitude of FTRL: It should balance the trade-off between the bias- and the variance-term. }}
		%
	\end{itemize}
\end{frame}


\begin{frame} 
	\frametitle{Online Gradient Descent:  Regret}
	\small
	\begin{itemize}
		%	
		%	
		%
		\item As a consequence, we can also derive a similar order of the regret for the OGD algorithm on OCO problems as for the FTRL on OLO problems by imposing a slightly different assumption on the (new) ``variance'' term $\sum\nolimits_{t=1}^T \norm{ \nabla_a \l(a_t^\OGD,z_t) }^2.$
		%	
		{\visible<2->{\item  \textbf{Corollary:}
		%	
		Suppose we use the OGD algorithm on an online convex optimization problem with a convex action space  $\Aspace \subset \mathbb{R}^d$ such that 
%		
		\begin{itemize}\small
%			
			\item $\sup_{\tilde a \in \Aspace}\norm{\tilde a} \leq B$ for some finite constant $B>0$
%			
			\item $\sup_{a\in \Aspace, z \in \Zspace}\norm{ \nabla_a \l(a,z) } \leq V$ for some finite constant $V>0.$
%			
		\end{itemize}
		%	
		Then, by choosing the step size $\eta$ for OGD as $\eta = \frac{B}{V\sqrt{2\, T}}$ we get
		%	
		$$	R_T^\OGD \leq   BV\sqrt{2\, T}.		$$ }}
		%	
		%	
	\end{itemize}
\end{frame}

\begin{frame}
%	
	\frametitle{Regret lower bounds for OCO}
	%     
	\small
	%	
	\begin{itemize}
		%    		
		\item   \textbf{Theorem.} For any online learning algorithm there exists an online convex optimization problem characterized by
%		
		\begin{itemize}\small
			\item  a convex loss function $\l,$
			\pause
			\item  a  bounded (convex) action space  $\Aspace= [-B,B]^d$ for some finite constant $B>0,$
			\pause
			\item  and bounded gradients $\sup_{a\in \Aspace, z \in \Zspace}\norm{ \nabla_a \l(a,z) } \leq V$  for some finite constant $V>0,$
		\end{itemize}
%		
		\pause 
		such that the algorithm incurs a regret of $\Omega(\sqrt{T})$ in the worst case.
		%		
		\pause \item Recall that  under (almost) the same assumptions as the theorem above, we have $R_T^\OGD \leq   BV\sqrt{2\, T}.$ 
		%    		
		\pause	\item [$\leadsto$] This result shows that the Online Gradient Descent 
		%		and the Exponentiated Gradient Descent algorithm are 
		is \emph{optimal} regarding its order of its regret with respect to the time horizon $T.$  
%		
	\end{itemize}
	%    
\end{frame}



%
%\section{Outlook}
%
%\begin{frame} {Online Machine Learning: Outlook}
%%	
%	Online machine learning is a very large field of research.	
%	%    
%	\begin{figure}
%		\centering
%		\includegraphics[width=0.99\linewidth]{figure/online_learning_overview}
%		\caption{Hoi et al. (2018), ''Online Learning: A Comprehensive Survey''.}
%	\end{figure}
%\end{frame}

%
\endlecture
\end{document}
