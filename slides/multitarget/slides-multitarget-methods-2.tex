\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-multitarget}

\usepackage{multicol}
\usepackage{color,colortbl} 
\definecolor{putblue}{RGB}{0,0,124}
\definecolor{putred}{RGB}{204,33,69}

\usetikzlibrary{mindmap,trees}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{decorations.shapes}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,
	decorations.pathreplacing,decorations.pathmorphing,shapes,matrix,shapes.symbols}
\usetikzlibrary{shapes.misc}


\newcommand{\titlefigure}{figure/Slide5}
\newcommand{\learninggoals}{
  \item Kronecker kernel ridge regression
  \item Graph relations in targets
  \item Probabilistic classifier chains
  \item Low-Rank approximations
  
%  \item 
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Multi-Target Prediction: Methods Part 2}
\lecture{Advanced Machine Learning}



\sloppy

%\section{Similarity-exploiting methods}


\begin{frame}{Kronecker kernel ridge regression}
	\begin{itemize}

    	\item In MTP with target features, we often use kernel methods.
    
    	\item Consider the following pairwise model representation in the primal: 
    		\begin{equation*}
    			\label{eq:pairwise}
    			f(\xv,\tv) = \bm\omega^\top \left(\phi(\xv) \otimes \psi(\tv) \right) ,
    		\end{equation*}
    
    	where $\phi$ is feature mapping for features and $\psi$ is feature mapping for target (features) and $\otimes$ is Kronecker product. 
    
    	\item This yields Kronecker product pairwise kernel in the dual:
    	
    	\begin{eqnarray*} 
    		f(\xv,\tv)= \sum_{(\xv',\tv') \in \D} \alpha_{(\xv',\tv')}  \cdot  k(\xv,\xv') \cdot g(\tv,\tv')  = \sum_{(\xv',\tv') \in \D} \alpha_{(\xv',\tv')} \Gamma((\xv,\tv),(\xv',\tv')),
    	\end{eqnarray*}
    
    	where $k$ is kernel for feature map $\phi,$  $g$ kernel for feature map $\psi$  and $\alpha_{(\xv',\tv')}$ are dual parameters determined by:
    
    	$$ \min_{\bm{\alpha}} \, ||\bm{\Gamma}\bm{\alpha} -\bm{z} ||^2_2 +\lambda\bm{\alpha }^\top \bm{\Gamma}\bm{\alpha}, \text{ where } \bm{z} = \mathrm{vec}{(Y)}$$
    
    
    %	\item Idea: Model the joint kernel as a product of an instance kernel $k(\cdot,\cdot)$ and a target kernel $g(\cdot,\cdot)$: 
    %		$$\Gamma((\xv,\tv),(\xv',\tv')) = k(\xv,\xv') \cdot g(\tv,\tv')$$
    
    	\item Commonly used in zero-shot learning.

\end{itemize}

	{\tiny Stock et al., A comparative study of pairwise learning methods based on kernel ridge regression, Neural Computation 2018.}
%	
\end{frame}



\begin{frame}{Exploiting relations in regularization}

	\begin{center}
		\includegraphics[width=0.65\textwidth]{figure/targetrelations}
	\end{center} 

	\begin{itemize}
		\item Graph-based regularization for graph-type relations in targets: 

		\begin{equation*}
			\min_\Theta \|Y - \Phi \Theta \|^2_F + \lambda \sum_{m=1}^l \sum_{m' \in \mathcal{N}(m)} \|\thetav_m - \ \thetav_{m'}\|^2,
		\end{equation*}

		where $\mathcal{N}(j)$ is the set of targets related to target $j.$
        \item The graph or tree is given as prior information.
		
		\item Can be extended to a weighted version aware of the similarities
	
	\end{itemize}

	{\tiny Gopal and Yang, Recursive regularization for large-scale classification with hierarchical and graphical dependencies, KDD 2013.}
\end{frame}

\begin{frame}{Hierarchical multi-label classification}
	
	\begin{minipage}{0.60\textwidth}  
        \centering
        \includegraphics[width=\textwidth]{figure/hloss}
    \end{minipage}
    \hfill
    \begin{minipage}{0.39\textwidth}    
        \centering
		\includegraphics[width=\textwidth,trim = 0 0 100 20,clip]{figure/Slide5}
    \end{minipage}
    
    \begin{itemize}
        \item  Hierarchies can also be used to define specific loss functions, such as the Hierarchy-loss: $$L_{Hier}(\yv, f) = \sum_{m: y_m \neq \hat{y}_m} c_m \, \mathds{1}_{ [\textit{anc}(y_m) = \textit{anc}(\hat{y}_m)]  },$$
        % where $c_m$ are costs depending on the depth of node $m$, and $\hat{y}_m$ are the predicted labels.
        \item This is rather common in multi-label classification problems.
    \end{itemize}
    
	{\tiny Bi and Kwok, Bayes-optimal hierarchical multi-label classification, IEEE Transactions on Knowledge and Data Engineering, 2014.}

\end{frame}

%\section{Similarity-constructing methods}

\begin{frame}{Probabilistic classifier chains}
	
	\begin{itemize}
		\item Estimate the \alert{joint} conditional distribution $\P(\yv ~|~  \xv)$. 
		\item For optimizing the \alert{subset 0/1} loss:  $$ L_{0/1}(\yv, \hat{\yv}) = \mathds{1}_{[\yv \ne \hat{\yv}]}$$
		\item Repeatedly apply the \emph{product rule} of probability:
		$$
		\P(\yv ~|~ \xv) = \prod_{j=m}^{l} \P(y_m ~|~ \xv, y_1, \ldots,y_{m-1}) \, .
		$$
		\item  Learning relies on constructing \alert{probabilistic classifiers} for
		$$
		\P( y_m|\xv, y_1, \ldots,y_{m-1}) \,,
		$$
		{independently} for each $m = 1, \ldots, l$. 
		%\item One can use scoring functions $f_i(\xv^\prime,y_i)$ and use logistic transformation.
		%\item By using the linear models, the overall scoring function takes the form:
		%$$
		%f(\xv,\yv) = \sum_{i=1}^m f_i(\xv, y_i) + \sum_{y_k,y_l} \! f_{k,l}(y_k,y_l) 
		%$$
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Probabilistic classifier chains}
	\begin{itemize}
		\item Inference relies on exploiting a probability tree:
	\end{itemize}
	\begin{center}
		\begin{tikzpicture}[scale = 0.85,every node/.style={scale=0.85},
			regnode/.style={circle,draw,minimum width=1.5ex,inner sep=0pt},
			leaf/.style={circle,fill=black,draw,minimum width=1.5ex,inner sep=0pt},
			pleaf/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt,line width=1pt},
			pnode/.style={rectangle,rounded corners=1ex,draw,font=\scriptsize,inner sep=3pt},
			rootnode/.style={rectangle,rounded corners=1ex,draw,font=\small,inner sep=4pt},
			level/.style={sibling distance=16em/#1, level distance=12ex}
			]
			\node (z) [rootnode] {$\xv$}
			child {node (a) [pnode] {$P(y_1=0 ~|~ \xv)=0.4$} 
				child {node [label=below:{\scriptsize \thickmuskip=0mu $P(\yv=(0,0)~|~ \xv)=0$}] (b) [pleaf] {\thickmuskip=-1.5mu $P(y_2=0 ~|~ y_1=0, \xv)=0.0$} edge from parent node[above left]{$y_2=0$}}
				child {node [label=below:{\color{blue} \scriptsize \thickmuskip=0mu $P(\yv=(0,1) ~|~ x)=0.4$}] (g) [pleaf] {\thickmuskip=-1.5mu $P(y_2=1 ~|~ y_1=0, \xv)=1.0$} edge from parent node[above right]{$y_2=1$}}
				edge from parent node[above left]{$y_1=0$}
			}
			child {node (j) [pnode] {$P(y_1=1 ~|~ \xv)=0.6$}
				child {node [label=below:{\scriptsize \thickmuskip=0mu $P(\yv=(1,0) ~|~ \xv)=0.24$}] (k) [pleaf] {\thickmuskip=-1.5mu $P(y_2=0 ~|~ y_1=1, \xv)=0.4$} edge from parent node[above left]{$y_2=0$}}
				child {node [label=below:{\color{red} \scriptsize \thickmuskip=0mu $P(\yv=(1,1) ~|~ \xv)=0.36$}] (l) [pleaf] {\thickmuskip=-1.5mu $P(y_2=1 ~|~ y_1=1, \xv)=0.6$}
					{
						child [grow=right] {node (s) {} edge from parent[draw=none]
							child [grow=up] {node (t) {} edge from parent[draw=none]
								child [grow=up] {node (u) {} edge from parent[draw=none]}
							}
						}
					}
					edge from parent node[above right]{$y_2=1$}
				}
				edge from parent node[above right]{$y_1=1$}
			};
			
			%\path (s) -- (t) node [midway] {$\lambda_{2}$};
			%\path (t) -- (u) node [midway] {$\lambda_{1}$};
		\end{tikzpicture}
	\end{center}
	
	\begin{itemize}\small
		\item For subset 0/1 loss one needs to find $
		h(\xv) = \argmax_{\yv} \P(\yv ~|~ \xv)
		$.
		\item Greedy and approximate search techniques with guarantees exist.
%		\footnote{Kumar et al., Beam search algorithms for multilabel
%	learning, Machine Learning 2013}
		\item Other losses: compute the prediction on a sample from $\P(\yv ~|~  \xv).$
	\end{itemize}
{\tiny Dembczynski et al., An analysis of chaining in multi-label classification, ECAI 2012.}
	
\end{frame}



\begin{frame}{Low-rank approximation}
	
	\begin{center}
		\includegraphics[width=9cm]{figure/lowrank}
	\end{center}

	\begin{itemize}
		\item Low rank = some structure is shared across targets
		\item Typically perform low-rank approx of param matrix:
		$$\min_\Theta \|Y - \Phi \Theta \|^2_F + \lambda \, \mathrm{rank}(\Theta)$$
	\end{itemize}
	{\tiny Chen et al., A convex formulation for learning shared structures from
	multiple tasks, ICML 2009.}
\end{frame}

\begin{frame}{Low-rank approximation}
	
	
	\begin{itemize}
		\item $\Theta$: parameter matrix of dimensionality $p \times l$ 
		\item $p$: the number of (projected) features
		\item $l$: the number of targets
		\item Assume a low-rank structure of $A$:\\
		$ \qquad  \qquad\qquad \quad  U  \quad \times \qquad  V \qquad \quad  =  \qquad  A$
		\begin{center}		
			\includegraphics[width=0.6\textwidth,trim = 0 0 0 0,clip]{figure/uv_decomp}
		\end{center}
		
		\item We can write $\Theta=UV$ and $\Theta \xv = UV \xv$
		\item $V$ is a $p \times \hat{l}$ matrix
		\item  $U$ is an $\hat{l} \times l$ matrix	
		\item $\hat{l}$ is the rank of $\Theta$
	\end{itemize}
	
\end{frame}

% \begin{frame}{Overview of methods}
	
% 	\begin{itemize}
% 		\item Popular for multi-output regression, multi-task learning and multi-label classification.
% 		\item Linear as well as nonlinear methods.
% 		\item Algorithms: 
% 		\begin{itemize}
% 			\item Principal component analysis, Canonical correlation analysis, Partial least squares.
% 			\item Singular value decomposition, Alternating structure optimization.
% 			\item Compressed sensing, Output codes, Landmark labels, Bloom filters, Auto-encoders.
% 		\end{itemize}
% 	\end{itemize}
%   \footnote{Weston et al., Kernel dependency estimation, NIPS 2002}
% 	\footnote{Multi-label prediction via sparse infinite CCA, NIPS 2009}
%\footnote{Tai and Lin, Multilabel classification with principal label space transformation, Neural Computation 2012}
%  	\footnote{Zhou et al., Clustered Multi-Task Learning Via Alternating Structure Optimization, NIPS 2011}
%	\footnote{Hsu et al., Multi-label prediction via compressed sensing. NIPS 2009}
%	\footnote{Zhang and Schneider, Multi-label Output Codes using Canonical Correlation Analysis, UAI 2011}
%	\footnote{Balasubramanian and Lebanon, The landmark selection method for multiple output
%	prediction, ICML 2012}
%	\footnote{Ciss\'e et al., Robust bloom filters for large multilabel
%	classification tasks, NIPS 2013}
% 	\footnote{Wicker et al., A nonlinear label compression and transformation
%	method for multi-label classification using autoencoders, PAKDD 2016}
%\end{frame}


%
\endlecture
\end{document}
