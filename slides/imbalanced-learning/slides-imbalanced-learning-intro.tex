\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg

\usepackage{multicol}

\newcommand{\titlefigure}{figure_man/imbalanced_data_plot_title}
\newcommand{\learninggoals}{
  \item What are imbalanced data sets
  \item Disadvantage of accuracy on imbalanced data sets
  \item Overview of techniques for handling imbalanced data sets
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Introduction to Imbalanced Learning}
\lecture{Advanced Machine Learning}



\sloppy

\begin{vbframe}{Imbalanced Data Sets}
%		    
            \begin{itemize}
		        \item Class imbalance: the occurrences of the classes are significantly different. 		     
		        \item Consequence: undesirable predictive behavior.	    
		        \item 	Example:
		        \begin{itemize}
                \small    
		            \item Sampling from two Gaussian distributions 
		        \end{itemize}		    
		    \end{itemize}
		   
			\begin{figure}
				\centering
				\includegraphics[width=0.7\linewidth]{figure_man/combined_data_plots.jpg}
			\end{figure}
%
\end{vbframe}

\begin{vbframe}{Imbalanced Data Sets: Benchmark}
%
			\begin{figure}
				\centering
				\includegraphics[width=0.9\linewidth]{figure_man/benchmark_plots.png}
			\end{figure}
			
			\begin{itemize}
			\footnotesize
                \item Train classifiers with 10-fold cv (stratified). Hold negative class constant at 10000. For the positive class, consider sizes of 10000, 1000, 100 and 50. Performance measures are calculated on the aggregated predictions of the testsets.
			\end{itemize}

%
\end{vbframe}

\begin{vbframe}{Imbalanced Data Sets: Examples}
    \small
    \begin{table}[h]
        \scriptsize
        \centering
        \begin{tabular}{cccc}
            \toprule
            \textbf{Domain} & \textbf{Task} & \textbf{Majority Class} & \textbf{Minor Class} \\ [5pt]
            \hline
            Medicine & Predict tumor pathology & Benign &  Malignant \\ [5pt]
            Information retrieval & Find relevant items & Irrelevant items & Relevant items \\ [5pt]
            Tracking criminals & Detect fraud emails & Non-fraud emails & Fraud emails \\ [5pt]
            Weather prediction & Predict extreme weather & Normal weather & Tornado, hurricane \\
            \hline
        \end{tabular}
    \end{table}
    
	\begin{itemize}
        \item In binary classification, the minority class is usually the positive class, while the majority class is the negative. 
        \item The positive class is oftentimes the more important one in real-world applications.
        \item Recall that imbalanced data sets can also be a source of bias related to the concept of fairness in ML, e.g.\ more data on white recidivism outcomes than for blacks.
	\end{itemize}

\end{vbframe}

\frame{
\frametitle{Issues with Evaluating Classifiers}

\begin{itemize}
    \item Ideal case: correctly classify as many instances as possible \\ $\Rightarrow$ High accuracy,	preferably $100\%$.
    \vspace{10pt}
    
	\item In practice, we often obtain on imbalanced data sets:
	\begin{itemize}
		\small	
		\item a \textbf{good} accuracy on the \textbf{majority} class(es),		
		\item a \textbf{poor} accuracy on the \textbf{minority} class(es). 	
	\end{itemize}
    \vspace{10pt}

	\item Reason: the classifier is biased towards the \textbf{majority} class(es), as predicting the majority class pays off in terms of accuracy.
    \vspace{10pt}

	\item Focusing only on the overall accuracy can have serious consequences. 
    \vspace{10pt}

\end{itemize}
%
}

\begin{vbframe}{Issues with Evaluating Classifiers}
	%
	\begin{itemize}
        
        \item Example:
		\begin{itemize}
			\small
			\item Assume that only 0.5\% of the patients have the disease,	
			\item Always predicting ``no disease'' $\leadsto$ accuracy of 99.5\% \\	
			$\leadsto$ Every patient is sent back home!		
		\end{itemize}
        \vspace{20pt}
        
		\item Ideal performance metric: the learning is \emph{properly} biased towards the minority class(es).	
        \vspace{20pt}

        \item Imbalance-aware performance metrics:
        \begin{itemize}
            \item G-score
            \item Balanced accuracy
            \item Matthews Correlation Coefficient
            \item Weighted macro $F_1$ score
        \end{itemize}
	\end{itemize}
\end{vbframe} 


\begin{vbframe}{Training Classifiers on Imbalanced Datasets}
\small
\begin{table}[h]
    \centering
    \begin{tabular}{p{0.2\textwidth} p{0.35\textwidth} p{0.35\textwidth}}
        \toprule
        \textbf{Approach} & \textbf{Main idea} & \textbf{Remark} \\ [5pt]
        \hline
        Algorithm-level & Bias classifiers towards minority & Special knowledge about classifiers is needed \\ [5pt]
        \hline
        Data-level & Re-balance the classes by resamping & No modification of classifiers is needed \\ [5pt]
        \hline
        Cost-sensitive Learning & Introduce different costs for misclassification when learning & Between algorithm- and data-level approaches \\ [15pt]
        \hline
        Ensemble-based & Ensemble learning plus one of three techniques above & - \\ [5pt]
        \bottomrule
    \end{tabular}
\end{table}

\end{vbframe}



\endlecture
\end{document}
