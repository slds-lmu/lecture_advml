\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg

\usepackage{multicol}

\newcommand{\titlefigure}{figure_man/imbalanced_data_plot_title}
\newcommand{\learninggoals}{
  \item Know what an imbalanced data set is
  \item Understand disadvantage of accuracy on imbalanced data
  \item Know techniques for handling imbalanced data sets
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning: Introduction}
\lecture{Advanced Machine Learning}



\sloppy

\begin{vbframe}{Imbalanced Data Sets}
%		    
            \begin{itemize}
		        \item Class imbalance: Ratio of classes is significantly different. 		     
		        \item Consequence: Undesirable predictive behavior for smaller class.	    
		        \item Example: Sampling from two Gaussian distributions 
		    \end{itemize}
		   
			\begin{figure}
				\centering
				\includegraphics[width=\linewidth]{figure_man/combined_data_plots.jpg}
			\end{figure}
%
\end{vbframe}

\begin{vbframe}{Imbalanced Data Sets: Examples}
    \small
    \begin{table}[h]
        \scriptsize
        \centering
        \begin{tabular}{llll}
            \toprule
            \textbf{Domain} & \textbf{Task} & \textbf{Majority Class} & \textbf{Minor Class} \\ [5pt]
            \hline
            Medicine & Predict tumor pathology & Benign &  Malignant \\ [5pt]
            Information retrieval & Find relevant items & Irrelevant items & Relevant items \\ [5pt]
            Tracking criminals & Detect fraud emails & Non-fraud emails & Fraud emails \\ [5pt]
            Weather prediction & Predict extreme weather & Normal weather & Tornado, hurricane \\
            \hline
        \end{tabular}
    \end{table}
    
	\begin{itemize}
        \item Often, the minority class is the more important class.
        \item Imbalanced data can be a source of bias related to concept of fairness. % in ML, e.g.\ more data on white recidivism outcomes than for blacks.
	\end{itemize}

\end{vbframe}

\frame{
\frametitle{Issues with Evaluating Classifiers}

\begin{itemize}
    \item Ideal case: correctly classify as many instances as possible \\ $\Rightarrow$ High accuracy,	preferably $100\%$.
   % \vspace{10pt}
    
	\item In practice, we often obtain on imbalanced data sets: \textbf{good} performance on the \textbf{majority} class(es), a \textbf{poor} performance on the \textbf{minority} class(es). 	
    %\vspace{10pt}

	\item Reason: the classifier is biased towards the \textbf{majority} class(es), as predicting the majority class pays off in terms of accuracy.
    %\vspace{10pt}

	\item Focusing only on accuracy can lead to bad performance on minority class. 
    %\vspace{10pt}
        \item Example:
		\begin{itemize}
			\small
			\item Assume that only 0.5\% of the patients have a disease,	
			\item Always predicting ``no disease'' leads to accuracy of 99.5\% %\\	$\Rightarrow$ Optimal policy would be no treating anybody		
		\end{itemize}
\end{itemize}
%
}

\begin{vbframe}{Issues with Evaluating Classifiers}
%
			\begin{figure}
				\centering
				\includegraphics[width=0.8\linewidth]{figure_man/benchmark_plots.pdf}
			\end{figure}
			
			
			\footnotesize
                In each scenario, we have 10.000 obs in the negative class. Number of obs in positive class varies between 10.000, 1.000, 100, and 50. Train classifiers with 10-fold stratified cv. Evaluate via aggregated predictions on test set.
			

%
\end{vbframe}

\begin{vbframe}{Possible Solutions}
	%
	\begin{itemize}
        
		\item Ideal performance metric: the learning is \emph{properly} biased towards the minority class(es).	
        %\vspace{20pt}

        \item Imbalance-aware performance metrics:
        \begin{itemize}
            \item G-score
            \item Balanced accuracy
            \item Matthews Correlation Coefficient
            \item Weighted macro $F_1$ score
        \end{itemize}
	\end{itemize}
\end{vbframe} 


\begin{vbframe}{Possible Solutions}
%\small
\begin{table}[h]
\footnotesize
    \centering
    \begin{tabular}{p{0.2\textwidth} p{0.4\textwidth} p{0.35\textwidth}}
        \toprule
        \textbf{Approach} & \textbf{Main idea} & \textbf{Remark} \\ [5pt]
        \hline
        Algorithm-level & Bias classifiers towards minority & Special knowledge about classifiers is needed \\ [5pt]
        \hline
        Data-level & Rebalance classes by resampling & No modification of classifiers is needed \\ [5pt]
        \hline
        Cost-sensitive Learning & Introduce different costs for misclassification when learning & Between algorithm- and data-level approaches \\ [15pt]
        \hline
        Ensemble-based & Ensemble learning plus one of three techniques above & - \\ [5pt]
        \bottomrule
    \end{tabular}
\end{table}

\end{vbframe}



\endlecture
\end{document}
