\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\sens}{\mathbf{A}} % vector x (bold)
\newcommand{\ba}{\mathbf{a}}
\newcommand{\batilde}{\tilde{\mathbf{a}}}
\newcommand{\Px}{\mathbb{P}_{x}} % P_x
\newcommand{\Pxj}{\mathbb{P}_{x_j}} % P_{x_j}
\newcommand{\indep}{\perp \!\!\! \perp} % independence symbol
% ml - ROC
\newcommand{\np}{n_{+}} % no. of positive instances
\newcommand{\nn}{n_{-}} % no. of negative instances
\newcommand{\rn}{\pi_{-}} % proportion negative instances
\newcommand{\rp}{\pi_{+}} % proportion negative instances
% true/false pos/neg:
\newcommand{\tp}{\# \text{TP}} % true pos
\newcommand{\fap}{\# \text{FP}} % false pos (fp taken for partial derivs)
\newcommand{\tn}{\# \text{TN}} % true neg
\newcommand{\fan}{\# \text{FN}} % false neg

\usepackage{multicol}

\newcommand{\titlefigure}{figure/SMOTE.png}
\newcommand{\learninggoals}{
	\item Know the idea of sampling methods for coping with imbalanced data
	\item Understand the different undersampling techniques
	\item Understand the state-of-art oversampling technique SMOTE
}

\title{Advanced Machine Learning}
\date{}

\begin{document}
	
	\lecturechapter{Imbalanced Learning: Sampling Methods}
	\lecture{Advanced Machine Learning}
	
	
	
	\sloppy
	
	
	
	\begin{frame}{Sampling Methods: Overview}
		\footnotesize
		\begin{itemize}

			\item Main idea: manipulate the distribution of the training examples (make it more balanced) to improve the performance on the minority classes.
			
			\item Advantage: independent of the underlying classifier $\leadsto$ very flexible and general.
   
			\item Three groups: 
            \vspace{10pt}
		
			\begin{minipage}{0.5\textwidth}
		
				\begin{itemize} 
                    \footnotesize
                    
					\item Undersampling --- Eliminating/Removing instances of the majority class(es) in the original data set.
                    \vspace{10pt}
			
					\item Oversampling --- Adding/Creating new instances of the minority class(es) to the original data set.
                    \vspace{10pt}

                    \item Oversampling is slower than undersampling, but oversampling usually works better.
                    \vspace{10pt}

					\item Hybrid approaches --- Combining undersampling and oversampling.
                    \vspace{10pt}
			
				\end{itemize}
		
			\end{minipage}
			\begin{minipage}{0.4\textwidth}
					\begin{figure}
					\centering
					\scalebox{0.8}{\includegraphics{figure/under_oversampling.png}}
				\end{figure}
			\end{minipage}
	
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}{Random Undersampling/Oversampling}
		\begin{small}
		\begin{itemize}
            \item Random oversampling (ROS):
            \begin{itemize}
                \item Expand the minority.
                \item Randomly \textbf{replicate} instances from the \textbf{minority} class(es) until a desired imbalance ratio is reached.
                \item Prone to overfitting due to multiple tied instances!
            \end{itemize}

            \item Random undersampling (RUS):
            \begin{itemize}
                \item Shrink the majority.
                \item Randomly \textbf{eliminate} instances from the \textbf{majority} class(es) until a desired imbalance ratio is reached.
                \item Might remove informative instances and destroy the important concepts in the data!
            \end{itemize}
	
			\item Better: Introduce heuristics in the removal process (RUS) and do not create exact copies (ROS).
		
			\end{itemize}
		\end{small}	

	\end{frame}
	
	\begin{frame}{Undersampling: Tomek Links}
		\footnotesize{

        \begin{itemize}            
            \item Key idea: remove only noisy borderline examples of the majority class(es).
            \item Noisy borderline examples:
            \begin{itemize}
                \footnotesize
                \item From different classes.
                \item ``Very close'' to each other.
            \end{itemize} 
                
        \end{itemize}
        \begin{columns}
            \begin{column}{0.5\textwidth}	
        
                \begin{itemize}
                    \item Let $E^{(i)} = (\xi,\yi)$ and $E^{(j)} = (\xv^{(j)},y^{(j)})$ be two data points in $\D$ with $\yi\neq y^{(j)}.$
                    \vspace{10pt}
                    
                    \item A pair $(E^{(i)},E^{(j)})$ is called \emph{Tomek link} iff there is no other data point $E^{(k)} = (\xv^{(k)},y^{(k)})$ such that
    
                    \begin{itemize} 
                    \footnotesize
                
                       \item [] $d(\xv^{(i)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $
                       \item [] or $d(\xv^{(j)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $ holds,
                    
                    \end{itemize}
    
                where $d$ is some distance on $\Xspace.$
                
                \end{itemize}		
            \end{column}

            \begin{column}{0.4\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=0.85\textwidth]{figure/tomek_link_plot.png}	
                    \tiny
                    \\ Franciso Herrera (2013), Imbalanced Classification: Common
                    Approaches and Open Problems (\href{https://sci2s.ugr.es/sites/default/files/files/TutorialsAndPlenaryTalks/SSTiC-Trends in-Classification-Imbalanced-data-sets.pdf}{\underline{URL}}).
                \end{figure}
            \end{column}
        \end{columns}
	   }
	\end{frame}

 \begin{frame}{Undersampling: Tomek Links}
    \footnotesize{
        \begin{columns}
            \begin{column}{0.5\textwidth}
                \begin{itemize} 
                    \footnotesize
                    
                     \item A pair $(E^{(i)},E^{(j)})$ is called \emph{Tomek link} iff there is no other data point $E^{(k)} = (\xv^{(k)},y^{(k)})$ such that
    
                    \begin{itemize} 
                    \footnotesize
                
                       \item [] $d(\xv^{(i)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $
                       \item [] or $d(\xv^{(j)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $ holds,
                    
                    \end{itemize}
                    \vspace{10pt}
                    
                    \item Since $E^{(i)}$ and $E^{(j)}$ have different $y$'s they correspond to a bordeline case.
                    \vspace{10pt}
				
                    \item Remove each data pair in a Tomek link, where the $y$ belongs to a majority class $\leadsto$ shrink the majority class(es).
                    \vspace{10pt}
    
                    \item Note that we do not sample here, but this approach can be combined with RUS.
                    \vspace{10pt}
                
                \end{itemize}
            \end{column}

            \begin{column}{0.4\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=0.85\textwidth]{figure/tomek_link_plot.png}	\tiny
                    \\ Franciso Herrera (2013), Imbalanced Classification: Common
                    Approaches and Open Problems (\href{https://sci2s.ugr.es/sites/default/files/files/TutorialsAndPlenaryTalks/SSTiC-Trends in-Classification-Imbalanced-data-sets.pdf}{\underline{URL}}).
                \end{figure}
            \end{column}
        \end{columns}
    }
 

 \end{frame}
	
	
	
	\begin{frame}{Undersampling: Condensed Nearest Neighbor (CNN)}
	
	    \footnotesize{
		\begin{itemize}
		
			\item Motivation: Remove instances from the majority class(es) which are far away from the decision boundary. 
            \item Core idea: constructing a consistent subset $\tilde{\D}$ of $\D$ in terms of the 1-NN classifier. 

			\item A subset $\tilde{\D}$ of $\D$ is called consistent if using a 1-NN classifier on $\tilde{\D}$ classifies each instance in $\D$ correctly.
	
		\end{itemize}

        \begin{figure}
            \centering
            \scalebox{0.4}{\includegraphics{figure/CNN_plot.png}}			\tiny
            \\ Franciso Herrera (2013), Imbalanced Classification: Common
            Approaches and Open Problems (\href{https://sci2s.ugr.es/sites/default/files/files/TutorialsAndPlenaryTalks/SSTiC-Trends in-Classification-Imbalanced-data-sets.pdf}{\underline{URL}}).
        \end{figure}	
	   }
    \end{frame}

    \begin{frame}{Undersampling: Condensed Nearest Neighbor (CNN)}
    \footnotesize{
        \begin{itemize}
            \item Creates a consistent subset:
            
            \begin{enumerate} \footnotesize
    
                \item Initialize $\tilde{\D}$ by selecting \textbf{all minority} class instances and by picking \textbf{one majority} class instance at random.
                \vspace{5pt}
        
                \item Classify each instance in $\D$ with the 1-NN classifier based on $\tilde{\D}.$
                \vspace{5pt}
            
                \item Remove all misclassified instances from $\D$ to $\tilde{\D}.$
                
            \end{enumerate}	

        \end{itemize}

        \begin{figure}
            \centering
            \scalebox{0.4}{\includegraphics{figure/CNN_plot.png}}			\tiny
            \\ Franciso Herrera (2013), Imbalanced Classification: Common
            Approaches and Open Problems (\href{https://sci2s.ugr.es/sites/default/files/files/TutorialsAndPlenaryTalks/SSTiC-Trends in-Classification-Imbalanced-data-sets.pdf}{\underline{URL}}).
        \end{figure}
        
    }
        
        
    \end{frame}
	
	\begin{frame}{Undersampling: Other Approaches}

		\footnotesize
		\begin{itemize}
	
			\item Neighborhood cleaning rule (NCL):
		
			\begin{enumerate} \footnotesize
				
				\item Find the three nearest neighbors for each instance $(\xi,\yi)$ in $\D.$
                \vspace{5pt}
			
				\item If $\yi$ belongs to the majority class \emph{and} the 3-NN classify it to be a minority class $\leadsto$ Remove $(\xi,\yi)$ from $\D.$
                \vspace{5pt}
			
				\item If $\yi$ belongs to the minority class \emph{and} the 3-NN classify it to be a majority class $\leadsto$ Remove the three nearest neighbors from $\D.$
                \vspace{5pt}
			
			\end{enumerate} 
            \vspace{10pt}
		
			\item One-sided selection (OSS): Tomek link + CNN
            \vspace{10pt}
	
			\item CNN + Tomek link: to reduce computation of finding Tomek links $\leadsto$ first use CNN and then remove the Tomek links.
            \vspace{10pt}
		
			\item Clustering approaches: Class Purity Maximization (CPM) and Undersampling based on Clustering (SBC).

		\end{itemize}

	\end{frame}
	
	\begin{frame}{Oversampling: SMOTE}
		%	
		\footnotesize
		\begin{itemize}
			%			
			\item The Synthetic Minority Oversampling TEchnique (SMOTE) operates by creating \textbf{new synthetic examples} of minority class.

			\item Idea: interpolate between several closely located minority examples.

			\item Note that the examples are created in the feature space $\Xspace$ rather than in the data space $\Xspace\times\Yspace.$
	
			\item Algorithm: For each minority class example 

			\begin{itemize} 
                \footnotesize

				\item Find its $k$ nearest minority neighbors.
		
				\item Randomly select \textbf{one} of these neighbors.
		
				\item Randomly generate new instances along the lines connecting the minority example and its selected neighbor.
		
			\end{itemize}

		\end{itemize}
%	
		\includegraphics{figure/SMOTE.png} 
		%	
	\end{frame}
	
	\begin{frame}{SMOTE: Generating new examples}
		%	
		\footnotesize
		
			\begin{itemize}
	
				\item Let $\xi$ be the feature of the minority instance and let $\xv^{(j)}$ be its nearest neighbor. The line connecting the two examples is
				
				$$		(1-\lambda) \xi + \lambda\xv^{(j)} = \xi + \lambda(\xv^{(j)} - \xi)	$$
%				
				where $\lambda \in [0,1].$
				%			
				\item Thus, by sampling randomly a $\lambda \in [0,1],$ say $\tilde{\lambda},$ we can create a new example on the connecting line via
%				
				$$   \tilde{\xv}^{(i)} =  \xi + \tilde{\lambda}(\xv^{(j)} - \xi)	 $$
				%	
			\end{itemize}		
				
				Example: Let $\xi = (1,2)^\top$ and $\xv^{(j)} = (3,1)^\top.$ Assume we draw $\tilde{\lambda} \approx 0.25.$
				%
			\begin{figure}
				\centering
				\includegraphics[width=0.8\linewidth]{figure_man/coordinate_system}
			\end{figure}
		%			
		
		%	
	\end{frame}

		
	\begin{frame}{SMOTE: Example}
		%	
		\footnotesize
		
		\begin{itemize}
			%			
			\item Let us consider the iris data set with $\Yspace=\{ \texttt{setosa},\texttt{versicolor},\texttt{virginica}  \}$ with 50 examples for each class.
			
			\item Make the data set ``imbalanced'': 
            \begin{itemize}
                \footnotesize
                \item relabel one class as positive
                \item relabel two other classes as negative
            \end{itemize}
            
			\item Running SMOTE with different $k$'s leads to the following:
			%	
		\end{itemize}		
%		
		%
		\begin{figure}
			\centering
			\includegraphics[width=0.8\linewidth]{figure_man/smoted_iris_data.jpeg}
		\end{figure}
		%			
		We created now (minority) data points which make it now slightly harder to separate the two classes with a separating hyperplane.
		%	
	\end{frame}


	\begin{frame}{SMOTE: Dis-/Advantages}

		\small

		\begin{itemize}
		
			\item SMOTE generalizes the decision region for the minority class instead of making it quite specific such as by random oversampling. 
            \vspace{10pt}
		
			\item SMOTE is, however, prone to overgeneralizing as it generalizes the minority area without paying attention to the majority class(es).
            \vspace{10pt}

		
			\item Nevertheless, SMOTE is still well-performed among the oversampling techniques and is the basis for many oversampling methods: Borderline-SMOTE, LN-SMOTE, $\ldots$ (over 90 extensions!)
            \vspace{10pt}

			\item A quite common approach is also to combine SMOTE with undersampling techniques such as Tomek link or NCL.

		\end{itemize}		

	\end{frame}


	\endlecture
\end{document}
