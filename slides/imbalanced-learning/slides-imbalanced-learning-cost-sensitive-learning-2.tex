\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


%\usepackage{algorithm}
%\usepackage{algorithmic}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Empirical thresholding
  \item Model-agnostic MetaCost 
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning:\\
Cost-Sensitive Learning Part 2}
\lecture{Advanced Machine Learning}



\sloppy



\begin{vbframe}{Empirical Thresholding: binary case}
    \begin{itemize}
        \item Theoretical threshold from MECP not always best, due to e.g. wrong model class, finite data, etc.
        \item Simply measure costs on data with different thresholds
        \item Then pick best threshold (Fig.1 in \href{https://www.aaai.org/Library/AAAI/2006/aaai06-076.php}{\beamergotobutton{Sheng et al. 2006}}):
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.95\textwidth]{slides/imbalanced-learning/figure/threshold_adjusting.png}
        \end{figure}

        %\item The optimal threshold $T^{*}$ corresponds to the point with the smallest $M_C$.

        \item What if two equal local minima? We prefer the one with wider span %(e.g. $T_2$ in the 3rd subfigure). Because it is less sensitive to small changes in $T$.

        \item Do this on validation data / over cross-val to avoid overfitting!
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Empirical Thresholding: binary case}
    \begin{itemize}
        \item Example: German Credit task
        \footnotesize{
        \begin{center}
                            \begin{tabular}{cc|cc}
        			& &\multicolumn{2}{c}{True class} \\
        			& & $y=$ good & $y=$ bad  \\
        			\hline
        			\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}} & $\hat y$ = good & 0 & 3 \\
        			& $\hat y$ = bad & 1 & 0\\
                \end{tabular}
        \end{center}
        }
        \item Theoretical: $C(good,bad)/(C(bad,good)+C(good,bad))=3/4=c^{*}$ 

        \item Empirical version with 3-CV: For XGBoost, empirical minimum deviates substantially from theoretical version

                \begin{figure}[h]
            \centering
            \includegraphics[width=0.95\textwidth]{slides/imbalanced-learning/figure_man/threshold_plots.pdf}
        \end{figure}

    \end{itemize}
\end{vbframe}


\begin{vbframe}{Empirical Thresholding: Multiclass}
    \begin{itemize}
 %        \item It is also possible to perform threshold adjusting in multi-class classifcation.
        
  %      \item Here, we present a simple approach for classifiers that output scores $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ in multi-class classification:
        
            \item In the standard setting, we predict class $h(\xv) = \argmaxlim_{k} \pikx$.
            
            \item Let's use $g$ thresholds $c_k$ now 
            
            \item Re-scale scores $ \mathbf{s} = (\frac{\pi(\xv)_1}{c_1},\ldots, \frac{\pi(\xv)_g}{ c_g})^\top$, 
            \item Predict class $\argmaxlim_k \pikx $.

            \item Compute empirical costs over cross-validation

            \item Optimize over $g$ (actually: $g-1$) dimensional threshold vector $(c_1, \ldots, c_g)^T$ to produce minimal costs

    \end{itemize}
\end{vbframe}

\begin{vbframe}{MetaCost: Overview}

        \begin{itemize}
            \item Model-agnostic wrapper technique                      
            \item General idea: 
                \begin{enumerate}
                \small
                    \item Relabel train obs with their low expected cost classes
                   
                    \item Apply classifier to relabeled data
                \end{enumerate} 
                \item Example German Credit task:
                                \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{figure_man/relabeling_viz.pdf}
        \end{figure}
                \item Relabeled instances colored red\
                \item Relabeling from good to bad more common because of costs
        \end{itemize}

\end{vbframe}


\begin{vbframe}{MetaCost: Algorithm}
	
		
% 	The procedure of MetaCost is divided into three phases:
			%			
		% \begin{minipage}{0.59\textwidth} 
		% 		\begin{itemize}
		% 			\item \textcolor{teal}{Bagging --- Train $B$ times on bootstrapped data}
  %    				\item \textcolor{blue}{Relabel --- Predict $\xi$ with classifiers that had it OOB and average}

  %                       \item \textcolor{blue}{ Relabel --- Get new class by MECP}
		% 			\item \textcolor{orange}{Cost-sens ---  Train on relabeled data}
		% 		\end{itemize}
		
		% \end{minipage}
			\begin{algorithmic}
				
				\footnotesize
%				
				\State \textbf{Input:} 
				$\D = \{(\xi,\yi)\}_{i=1}^n$ training data, $B$ number of bagging iterations,
				$\pix$ probabilistic classifier,
				$\mathbf{C}$ cost matrix \\
				 \textcolor{teal}{\# Bagging: Classifier is trained on different bootstrap samples.
\For{$b=1,\ldots,B$}
    \State $\D_b \leftarrow $ Bootstrap version of $\D$
    \State $\pi_b \leftarrow $ train classifier on $\D_b$ 
\EndFor 
\\
\textcolor{blue}{\# Relabeling: Find classifiers for which $\xi$ is OOB and compute $\pi_b$ by averaging over predictions.
Determine new label $\tilde y^{(i)}$ w.r.t. to the cost minimal class.
\For{$i=1,\ldots,n$}
 	 \State $\tilde M \leftarrow \bigcup_{m: \xi \notin \D_m} \{m\}$ 
\EndFor
\For{$j=1,\ldots,g$} 
    \State $\pi_j(\xi)  \leftarrow \frac{1}{|\tilde M| } \sum_{m \in \tilde M}   \pi_j(\xi~|~ f_m) $
\EndFor
\\
$\tilde y^{(i)} \leftarrow \argmin_{i} \sum_{j=1}^g \pi_j(\xi) C(i,j) $\\
$\tilde D \leftarrow \tilde D \cup \{(\xi,\tilde y^{(i)})\} $
}}  
\\
\textcolor{orange}{\# Cost Sensitivity: Train on relabeled data.\\
$f_{meta} \leftarrow$ train $f$ on $\tilde D$}
\end{algorithmic}

\end{vbframe}

\endlecture
\end{document}
