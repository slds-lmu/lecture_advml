\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval.tex}

%\usepackage{algorithm}
%\usepackage{algorithmic}

% lowr case c vector used only here, was undefined, not in latex-math
\newcommand{\cv}{\mathbf{c}}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Instance specific costs
  \item Cost-Sensitive OVO
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning:\\
Cost-Sensitive Learning Part 3}
\lecture{Advanced Machine Learning}



\sloppy

%%%%%



%%%
% reference: https://www.csie.ntu.edu.tw/~htlin/talk/doc/csovo.acml14.handout.pdf

\begin{vbframe}{Binary Instance-specific cost Learning}
    \begin{itemize}
        \item Assumes instance-specific costs for every observation:  $\D^{(n)} = \{(\xi, \cv^{(i)})\}_{i=1}^n$, where $(\xi, \cv^{(i)}) \in \mathbb{R}^p \times \mathbb{R}^2$.

        \item Define ``true class'' as cost minimal class
        
        \item Define observation weights: $|\cv^{(i)}[1] - \cv^{(i)}[0]|$
        % Consider four instances, where $\cv^{(i)}[0]$ and $\cv^{(i)}[1]$ denote the instance-wise cost of classifying an example as 0 or 1 respectively, $\yi$ the true label and 

        \begin{center}
            \begin{tabular}{cc|cccc}\
        			& & $\cv^{(i)}[0]$ & $\cv^{(i)}[1]$ & $\yi$ & $w^{(i)}$ \\
        			\hline & $\xv^{(1)}$ & 1 & 1 & 0 & 0\\
        			& $\xv^{(2)}$ & 1 & 2 & 0 & 1\\
        			& $\xv^{(3)}$ & 7 & 3 & 1 & 4\\
            \end{tabular}
        \end{center}

        \item Now solve weighted ERM:
        \begin{equation*}
            \mathcal{R}_{emp}(\boldsymbol{\theta}) = \sum_{i=1}^{n}w^{(i)} \Lxyit
        \end{equation*}
        
        \item NB: Instances with equal costs are effectively ignored.
        \end{itemize}
            
\end{vbframe}

\begin{vbframe}{Multiclass Costs}
    \begin{itemize}

        \item Consider $g > 2$. Vanilla CSL is special case of instance specific,\\use $\cv^{(i)}$ same for all $\xi$ of the same class
        %straightforward to see that class-specific cost-sensitive learning is a special case of instance-specific cost learning: 
        
        %\item Now consider a multiclass label space $\mathcal{Y}=\{1,2,3\}$. For class-specific cost sensitive learning, we may define the following cost matrix: 
        \vspace{5pt}
        \begin{center}
                            \begin{tabular}{cc|ccc}
        			& &\multicolumn{3}{c}{True class} \\
        			& & $y=1$ & \textcolor{blue}{$y=2$} & \textcolor{green}{$y=3$}  \\
        			\hline
        			\multirow{3}{*}{\parbox{0.6cm}{Pred.  class}} & $\hat y=1$ & 0 & \textcolor{blue}{1} & \textcolor{green}{3}\\
        			& $\hat y=2$ & 1 & \textcolor{blue}{0} & \textcolor{green}{1}\\
                        & $\hat y=3$ & 7 & \textcolor{blue}{1} & \textcolor{green}{0}\\
                \end{tabular}
        \end{center}
        \vspace{5pt}
        
        \item For two $\xi$ with $y=2$ and $y = 3$: %instances of class 2 and 3 respectively, we have that
                \vspace{5pt}

                \begin{center}
                            \begin{tabular}{cc|cccc}\
        			& & $\cv^{(i)}[1]$ & $\cv^{(i)}[2]$ & $\cv^{(i)}[3]$ & $\yi$ \\
        			\hline & $\xv^{(1)}$ & \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{1} & \textcolor{blue}{2}\\
        			& $\xv^{(2)}$ & \textcolor{green}{3} & \textcolor{green}{1} & \textcolor{green}{0} & \textcolor{green}{3}\\
                        & $\xv^{(3)}$ & \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{1} & \textcolor{blue}{2}\\
                \end{tabular}
        \end{center}

        \item Set $\cv^{(i)}[\yi] = 0$, i.e. zero-cost for correct prediction.
        \vspace{5pt}
        
   %     \item Is the multiclass case restricted to class-specific cost sensitive learning? Of course not! Binary instance-specific cost learning can easily be generalized to the multiclass case. 
            
    \end{itemize}
\end{vbframe}

\begin{vbframe}{CSOVO \href{https://proceedings.mlr.press/v39/lin14.pdf}{\beamergotobutton{Lin et al. 2014}}}
    

    \begin{itemize}
        \item Let $\D^{(n)} = \{(\xi, \cv^{(i)})\}_{i=1}^n$, $(\xi, \cv^{(i)}) \in \mathbb{R}^p \times \mathbb{R}^g$.  
        \item Example:
       % Consider instance-specific costs for three observations of the form $(\xi, \yi, \cv^{(i)})$, with some label space $\mathcal{Y}=\{1, 2, \ldots, g\}$:

                        \begin{center}
                            \begin{tabular}{cc|ccc}\
        			& & $\cv^{(i)}[1]$ & $\cv^{(i)}[2]$ & $\cv^{(i)}[3]$  \\
        			\hline & $\xv^{(1)}$ & 0 & 2 & 3\\
        			& $\xv^{(2)}$ & 1 & 0 & 1\\
                 	& $\xv^{(3)}$ & 2 & 0 & 3\\
                \end{tabular}
        \end{center}
        
        %\item Now: How to perform cost-sensitive learning when $g > 2$?
        
        \item Idea: Reduction principle to binary case (weighted fit) by one-versus-one (OVO). 
        %we have learnt cost-sensitive learning for the binary case. $\leadsto$ Convert the problem to 
        
        
        \item For class $j$ vs. $k$:
        %, consider two problems:
        \begin{itemize}
            \item How to deal with the label $\yi$? $\yi$ can be neither $j$ nor $k$.
            \vspace{5pt}
            
            \item How to deal with the costs $\cv^{(i)}[j]$ and $\cv^{(i)}[k]$?
            
        \end{itemize}
            
    \end{itemize}

\end{vbframe}


\begin{vbframe}{CSOVO}
    \begin{itemize}
        \item When training a binary classifier $f^{(j, k)}$ for class $j$ vs. $k$,
        \begin{itemize}
            \item Choose cost min class from pair $\argmin_{l \in \{j, k\}} \cv^{(i)}[l]$ as ground truth 
            
            \item Sample weight is simply diff between the 2 costs      $|\cv^{(i)}[j] - \cv^{(i)}[k]|$

            %\item After cost transformation, the misclassification costs across classes are equal to 1. $\leadsto$ Sample-wise weighted binary classification problem.
        \end{itemize}
        
        \item Example continued:
\begin{center}
    \footnotesize
                            \begin{tabular}{cc|ccc|ccc}\
        			& & $\cv^{(i)}[1]$ & $\cv^{(i)}[2]$ & $\cv^{(i)}[3]$ & $\cv^{(i)}[1 \ \text{vs} \ 2]$ & $\tilde{y}^{( i)}[1 \ \text{vs} \ 2]$ & $w^{(i)}[1 \ \text{vs} \ 2]$\\
        			\hline & $\xv^{(1)}$ & 0 & 2 & 3 & 0/2 & 1 & 2\\
        			& $\xv^{(2)}$ & 1 & 0 & 1 & 1/0 & 2 & 1 \\
                 	& $\xv^{(3)}$ & 2 & 0 & 3 & 2/0 & 2 & 2\\
                \end{tabular}

                \begin{tabular}{cc|ccc|ccc}
        			& & $\cv^{(i)}[1]$ & $\cv^{(i)}[2]$ & $\cv^{(i)}[3]$ & $\cv^{(i)}[2 \ \text{vs} \ 3]$ & $\tilde{y}^{( i)}[2 \ \text{vs} \ 3]$ & $w^{(i)}[2 \ \text{vs} \ 3]$\\
        			\hline & $\xv^{(1)}$ & 0 & 2 & 3 & 2/3 & 2 & 1\\
        			& $\xv^{(2)}$ & 1 & 0 & 1 & 0/1 & 2 & 1\\
                 	& $\xv^{(3)}$ & 2 & 0 & 3 & 0/3 & 2 & 3\\
                \end{tabular}
\end{center}

    \end{itemize}
\end{vbframe}


\begin{vbframe}{CSOVO}
    \begin{itemize}
    \item Example continued
    \begin{center}  
        \footnotesize{
            \begin{tabular}{cc|ccc|ccc}
                & & $\cv^{(i)}[1]$ & $\cv^{(i)}[2]$ & $\cv^{(i)}[3]$ & $\cv^{(i)}[1 \ \text{vs} \ 3]$ & $\tilde{y}^{( i)}[1 \ \text{vs} \ 3]$ & $w^{(i)}[1 \ \text{vs} \ 3]$\\
                \hline & $\xv^{(1)}$ & 0 & 2 & 3 & 0/3 & 1 & 3\\
                & $\xv^{(2)}$ & 1 & 0 & 1 & -/- & - & 0 \\
                & $\xv^{(3)}$ & 2 & 0 & 3 & 2/3 & 1 & 1\\
            \end{tabular}
        }
    \end{center}

    \item Wrap everything up:
    \begin{enumerate}
        \item For class $j$ vs. $k$, transform all $(\xi, \cv^{(i)})$ to $(\xi, \argmin_{l \in \{j, k\}} \cv^{(i)}[l])$ with sample-wise weight $|\cv^{(i)}[j] - \cv^{(i)}[k]|$.
        
        \item Train a weighted binary classifier $f^{(j, k)}$ using the above
        
        \item Repeat step 1 and 2 for different $(j, k)$.
        
        \item Predict using the votes from all $f^{(j, k)}$.
    \end{enumerate}

    \item Theoretical guarantee:\\ 
    test costs of final classifier $\leq 2\sum_{j < k}$ test cost of $f^{(j, k)}$. 
    \end{itemize}
\end{vbframe}


%\begin{vbframe}{MetaCost: Example}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item We compare C4.5 (decision tree) with MetaCost using C4.5 for the \href{http://staffwww.itn.liu.se/~aidvi/courses/06/dm/
%				labs/heart-c.arff}{heart data set}  in \href{ http://www.cs.waikato.ac.nz/ml/weka/}{Weka}. 
%			%		
%			\item The cost matrix $\mathbf{C}$ is 
%			
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
%					& $\hat y$ = -1 & $ 4 $              &  $0$   \\
%				\end{tabular}
%			\end{center}
%		
%			\item The resulting confusion matrices are 
%			
%						
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& MetaCost & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $104$                & $ 21 $\\
%					& $\hat y$ = -1 & $ 61 $              &  $117$   \\
%				\end{tabular}
%							\begin{tabular}{cc|cc}
%				& &\multicolumn{2}{c}{True class} \\
%				& C4.5 & $y=1$ & $y=-1$  \\
%				\hline
%				\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $138$                & $ 40 $\\
%				& $\hat y$ = -1 & $ 27$              &  $98$   \\
%			\end{tabular}
%			\end{center}
%		
%		
%			%		
%			\item The total cost of MetaCost is 145, while C4.5 has total costs of 187. However, MetaCost has $0.729$ correct classifications and C4.5 has $0.779.$ 
%			
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}



%\begin{vbframe}{Cost-Sensitive Decision Trees}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item 
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}


%
\endlecture
\end{document}
