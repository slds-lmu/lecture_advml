\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/f1_score_plot}
\newcommand{\learninggoals}{
  \item Get to know alternative performance measures for accuracy
  \item See their advantages over accuracy for imbalanced data sets
  \item Understand the extensions of these measures for multiclass settings
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning: Performance Measures}
\lecture{Advanced Machine Learning}



\sloppy

\begin{vbframe}{Recap: performance measures for binary classification}
    \footnotesize{
        \begin{itemize}
            \item We encourage readers to first go through \href{https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-08-measures-classification/}{\beamergotobutton{Chapter 04.08 in I2ML }}.
            \item In binary classification  (i.e., $\Yspace = \{-1,+1\}$):

		\end{itemize}
		
		\begin{center}
		\tiny
		\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{cc||cc|c}
			& & \multicolumn{2}{c|}{\bfseries True Class $y$} & \\
			& & $+$ & $-$ & \\ 
			\hline \hline
			\bfseries Classification     & $+$ & TP & FP & $\rho_{PPV} = \frac{\tp}{\tp + \fap}$\\
			$\yh$ & $-$ & FN & TN & $\rho_{NPV} = \frac{\tn}{\fan + \tn}$\\
			\hline
			& & $\rho_{TPR} = \frac{\tp}{\tp + \fan}$ & $\rho_{TNR} = \frac{\tn}{\fap + \tn}$ & $\rho_{ACC} = \frac{\tp+ \tn}{\text{TOTAL}}$
		\end{tabular}
		\renewcommand{\arraystretch}{1}
        \end{center}

        \begin{itemize}
            \item $F_1$ score balances Recall ($\rho_{TPR}$) and Precision ($\rho_{PPV}$):
            $$\rho_{F_1} = 2 \cdot \cfrac{\rho_{PPV} \cdot \rho_{TPR}}{\rho_{PPV} + \rho_{TPR}}$$
            
            \item Note $\rho_{F_1}$ does not account for TN.
            
            \item Question: does $F_1$ score suffer from data imbalance as accuracy?
        \end{itemize}
            
    }
    
\end{vbframe}


\begin{vbframe}{$F_1$ Score in Binary classification}
	\footnotesize{
	
    	\begin{minipage}[c]{0.5\textwidth}
    		$F_1$ is the \textbf{harmonic mean} of $\rho_{PPV}$ \& $\rho_{TPR}$. \\
    		$\rightarrow$ Property of harmonic mean: tends more towards the \textbf{lower} of two combined values.
    	\end{minipage}%
    	\begin{minipage}[c]{0.5\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figure/f1_score_plot.pdf}
    	\end{minipage}
	
    	\begin{itemize}
    		\item A model with $\rho_{TPR} = 0$ (no pos. instance predicted as 
    		pos.) or 
    		$\rho_{PPV} = 0$ (no TP among the predicted) has $\rho_{F_1} = 0$.
      
    		\item Always predicting \enquote{negative}: $\rho_{F_1} = 0$
      
    		\item Always predicting \enquote{positive}: $\rho_{F_1} = 2 \cdot \rho_{PPV} / 
    		(\rho_{PPV} + 1) = 2 \cdot \np / (\np + n)$,\\ 
    		$\leadsto$ small when $\np$ is small.
    
            \item Hence, $F_1$ score is more robust to data imbalance than accuracy.
      
    	\end{itemize}
	}
\end{vbframe}

\begin{vbframe}{$F_\beta$ in Binary classification}
    \footnotesize{
	
    	\begin{minipage}[c]{0.49\textwidth}
            \begin{itemize}
                \item $F_1$ puts equal weights to $\frac{1}{\rho_{PPV}}$ \& $\frac{1}{\rho_{TPR}}$ because $F_1 = \frac{2}{\frac{1}{\rho_{PPV}} + \frac{1}{\rho_{TPR}}}$.
                
                \item $F_\beta$ puts $\beta^2$ times of weight to $\frac{1}{\rho_{TPR}}$:
                    \begin{equation*}
                        \begin{aligned}
                            F_\beta &= \frac{1}{\frac{\beta^2}{1 + \beta^2} \cdot \frac{1}{\rho_{TPR}} + \frac{1}{1 + \beta^2} \cdot \frac{1}{\rho_{PPV}}} \\
                            &= (1 + \beta^2) \cdot \frac{\rho_{PPV} \cdot \rho_{TPR}}{\beta^2 \rho_{PPV} + \rho_{TPR}}
                        \end{aligned}
                    \end{equation*}
            \end{itemize}
    	\end{minipage}
    	\begin{minipage}[c]{0.49\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figure/f1_score_plot.pdf}
    	\end{minipage}
    	
    	\begin{itemize}
    		\item $\beta \gg 1 \ \leadsto \ F_\beta \approx \rho_{TPR}$;
            \item $\beta \ll 1 \ \leadsto \ F_\beta \approx \rho_{PPV}$.
      
    	\end{itemize}

    }
    
\end{vbframe}


\begin{vbframe}{G Score and G Mean}
	\footnotesize{
    	\begin{minipage}[c]{0.49\textwidth}
        	\begin{itemize}
        		\item G score uses geometric mean: 
        		$$\rho_{G} = \sqrt{\rho_{PPV} \cdot \rho_{TPR}}$$

        		\item Geometric mean tends more towards the \textbf{lower} of the two combined values.
                \item Geometric mean is \textbf{larger} than harmonic mean.
        	\end{itemize}
        \end{minipage}
        \begin{minipage}[c]{0.49\textwidth}
        	\centering
        	\includegraphics[width=0.8\textwidth]{figure/g_score_plot.pdf}
        \end{minipage}

        \begin{itemize}
        	\item Closely related is the G mean:
        	$$\rho_{Gm} = \sqrt{\rho_{TNR} \cdot \rho_{TPR}}.$$
        	It also considers \textbf{TN}.
    
        	\item Always predicting \enquote{negative}: $\rho_{G} = \rho_{Gm}  = 0 \leadsto$ Robust to data imbalance!
        \end{itemize}
}
\end{vbframe}



\begin{vbframe}{Balanced Accuracy}
	\footnotesize{
	
    	\begin{minipage}[c]{0.49\textwidth}
    		\begin{itemize}
    			\item Balanced accuracy (BAC) balances $\rho_{TNR}$ and $\rho_{TPR}$: 
    			$$\rho_{BAC} = \frac{\rho_{TNR} + \rho_{TPR}}{2}$$

    			\item  It tends more towards the \textbf{higher} of two combined values.
    		\end{itemize}
    	\end{minipage}
    	\begin{minipage}[c]{0.49\textwidth}
    		\centering
    		\includegraphics[width=0.8\textwidth]{figure/bac_plot.pdf}
    	\end{minipage}

    	\begin{itemize}
    		\item If a classifier attains high accuracy on both classes or the data set is almost balanced, then $\rho_{BAC} \approx \rho_{ACC}$.
            \vspace{20pt}
            
    		\item However, if a classifier always predicts ``negative'' for an imbalanced data set, i.e. $n_+  \ll n_-,$ then $\rho_{BAC} \ll \rho_{ACC}$. It also considers TN.
    
    	\end{itemize}
    }
\end{vbframe}


\begin{vbframe}{Matthews Correlation Coefficient}

	\small{
    	\begin{itemize}
    
            \item Recall: Pearson correlation coefficient (PCC): 
                $$\corr(X, Y) = \frac{\cov(X, Y)}{\sigma_{X} \sigma_{Y}}$$
    		\item View ``predicted'' and ``true'' classes as two binary random variables.
      
            \item Using entries in confusion matrix to estimate the PCC, we obtain MCC:
    	
    		$$   \rho_{MCC} = \frac{TP\cdot TN - FP \cdot FN}{\sqrt{(TP+FN)(TP+FP)(TN+FN)(TN+FP)}}$$
    
            \item In contrast to other metrics: 
            \begin{itemize}
                \footnotesize
                \item MCC uses all entries of the confusion matrix;
                \item MCC has value in $[-1,1]$.
            \end{itemize}
    		
        \end{itemize}
    	
	}
\end{vbframe}

\begin{vbframe}{Matthews Correlation Coefficient}
    \small{
        $$   \rho_{MCC} = \frac{TP\cdot TN - FP \cdot FN}{\sqrt{(TP+FN)(TP+FP)(TN+FN)(TN+FP)}}$$

        \begin{itemize}
            \item $\rho_{MCC} \approx 1$ $\leadsto$ nearly zero error $\leadsto$ good classification, i.e., strong correlation between predicted and true classes.
            \vspace{10pt}
    	
            \item $\rho_{MCC} \approx 0$ $\leadsto$ no correlation, i.e., no better than random guessing.
            \vspace{10pt}
    	
            \item $\rho_{MCC} \approx -1$ $\leadsto$ reversed classification, i.e., essentially switching the labels.
            \vspace{10pt}
    
            \item Previous measures requires defining positive class. But MCC does not depend on which class is the positive one.
            
        \end{itemize}
         
    }
    
\end{vbframe}


\begin{vbframe}{Performance Measures for Multiclass Classification}
	\small{
		\begin{center}
			\tiny
			\begin{tabular}{cc|>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{5em}>{\centering\arraybackslash}p{8em}}
				& & \multicolumn{4}{c}{\bfseries True Class $y$} \\
				& & $1$ & $2$ & $\ldots$ & $g$  \\
				\hline
				\bfseries Classification     & $1$ & $n_{11}$  &  $n_{12}$  & $\ldots$ &  $n_{1g}$ \\
				& & (True 1's) & (False 1's for 2's) & $\ldots$ &  (False 1's for $g$'s)  \\
				& $2$ &  $n_{21}$  &  $n_{22}$  & $\ldots$ & $n_{2g}$  \\
				$\yh$ & & (False 2's for 1's) & (True 2's) & $\ldots$ &  (False 2's for $g$'s)  \\
				& $\vdots$ & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ \\
				& $g$ & $n_{g1}$ & $n_{g2}$  & $\ldots$ &  $n_{gg}$\\
				& & (False $g$'s for 1's) & (False $g$'s for 2's) & $\ldots$ &  (True $g$'s)  \\
			\end{tabular}
		\end{center}
		
		\begin{itemize}
			\item $n_{ij}$: the number of $j$ instances classified as $i$.

            \item $n_i = \sum_{j=1}^g n_{ji}$ the total number of $i$ instances.

            \item \textbf{Class-specific} metrics:
                \begin{itemize}
                    \small
                    
                    \item True positive rate (\textbf{Recall}): $\rho_{TPR_i} = \frac{n_{ii}}{n_i}$
    
                    \item True negative rate $\rho_{TNR_i} = \frac{\sum_{j\neq i}n_{jj}}{n-n_i}$ 
    
                    \item Positive predictive value (\textbf{Precision}) $\rho_{PPR_i} = \frac{n_{ii}}{\sum_{j=1}^g n_{ij}}$.
                    
                \end{itemize}

		\end{itemize}
	}
\end{vbframe}


\begin{vbframe}{Macro $F_1$ Score}
	
	\small{

    	\begin{itemize}
    
    		\item Average over classes to obtain a single value:
            $$\rho_{mMETRIC} = \frac{1}{g}\sum_{i=1}^g \rho_{METRIC_i},$$
            where $METRIC_i$ is a class-specific metric such as $PPV_i$, $TPR_i$ of class $i$.
    		
    	
    		\item With this, one can simply define a \textbf{macro} $F_1$ score:
    	
    		$$\rho_{mF_1} = 2 \cdot \cfrac{\rho_{mPPV} \cdot \rho_{mTPR}}{\rho_{mPPV} + 
    			\rho_{mTPR}}$$
    
    		\item Problem: each class equally weighted $\leadsto$ class sizes are not considered.
    
            \item How about applying different weights to the class-specific metrics?

    	\end{itemize}
	}
\end{vbframe}

\begin{vbframe}{Weighted Macro $F_1$ Score}

	\small{

    	\begin{itemize}
    
    		\item For imbalanced data sets, give \textbf{more weights} to \textbf{minority} classes.
      
            \item $w_1,\ldots,w_g \in[0,1]$  such that $w_i > w_j$ iff $n_i < n_j$ and $\sum_{i=1}^g w_i = 1.$
    
            $$\rho_{wmMETRIC} = \frac{1}{g}\sum_{i=1}^g \rho_{METRIC_i} w_i,$$
            where $METRIC_i$ is a class-specific metric such as $PPV_i$, $TPR_i$ of class $i$.
    	 
    		\item Example: $w_i = \frac{n - n_i}{(g-1)n}$ are suitable weights.
    
    		\item Weighted macro $F_1$ score:	
    		$$\rho_{wmF_1} = 2 \cdot \frac{\rho_{wmPPV} \cdot \rho_{wmTPR}}{\rho_{wmPPV} + \rho_{wmTPR}}$$
      
    		\item This idea gives rise to a weighted macro G score or weighted BAC.
    	
    		\item \textbf{Usually}, weighted $F_1$ score uses $w_i = n_i/n$. However, for imbalanced data sets this would \textbf{overweight} majority classes.
    
    	\end{itemize}
	}
\end{vbframe}


\begin{vbframe}{Other Performance Measures}

	\small{

		\begin{itemize}
		
			\item ``Micro'' versions of metrics for the multiclass setting: e.g. the micro $TPR$ is $\frac{\sum_{i=1}^g TP_i}{\sum_{i=1}^g TP_i + FN_i}$ 
            \vspace{10pt}
		
			\item Moreover, the MCC can be extended to:
			
			$$   \rho_{MCC} = \frac{ n  \sum_{i=1}^g n_{ii} -  \sum_{i=1}^g \hat n_i n_i}{\sqrt{ (n^2 - \sum_{i=1}^g \hat n_i^2)(n^2 - \sum_{i=1}^g n_i^2)  }},$$

			where $\hat n_i = \sum_{j=1}^g n_{ij}$ is the total number of instances classified as $i.$
            \vspace{10pt}
        
		
			\item Cohen's Kappa or Cross Entropy (see Grandini et al.\ (2021)) treat "predicted" and "true" classes as two discrete random variables.
		
		\end{itemize}
	}
\end{vbframe}


\begin{vbframe}{Which Performance Measure to use?}

	\small{

		\begin{itemize}

            \item Since different measures focus on different characteristics $\leadsto$ No golden answer to this question.
	
			\item Depends on the application, which characteristic is more important than another.

			\item However, it is clear that the usage of accuracy is inappropriate if the data set is imbalanced. $\leadsto$ Use altenative ones.

			\item Be careful with comparing the absolute values of the different measures, as these can be on different ``scales'', e.g.\ MCC and BAC. 
	
		\end{itemize}
	}
\end{vbframe}



%
\endlecture
\end{document}
