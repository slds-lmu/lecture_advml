\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}

\newcommand{\titlefigure}{figure_man/cost-curves-2}
\newcommand{\learninggoals}{
\item Understand cost curves
\item As alternative to ROC curves
}


\title{Introduction to Machine Learning}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: Cost Curves Part 2}
\lecture{Introduction to Machine Learning}
\sloppy


% ------------------------------------------------------------------------------

\begin{vbframe}{consider costs}

Assume unequal misclassif costs, i.e., $cost_{FN} \neq cost_{FP}$ and generalize error rate to \textbf{expected costs} (as function of $\rp$): %consider expected costs as fucntion of $\rp$ as general form of error rate:
$$Costs(\rp) = (1 - \rp) \cdot FPR \cdot cost_{FP} + \rp \cdot FNR \cdot cost_{FN}$$
Maximum of expected costs happens when
$$FPR=FNR=1 \; \Rightarrow \; Costs_{max} = (1 - \rp) \cdot cost_{FP} + \rp \cdot cost_{FN}$$
Consider \textbf{normalized costs} (as function of $\rp$):
%$$\Rightarrow Costs_{norm}(\rp) = \tfrac{(1 - \rp) \cdot FPR \cdot cost_{FP} \; + \; \rp \cdot FNR \cdot cost_{FN}}{(1 - \rp) \cdot cost_{FP} \; + \; \rp \cdot cost_{FN}} \in [0,1]$$
%$$\Rightarrow Costs_{norm}(\rp) = \tfrac{(1 - \rp) \cdot cost_{FP} \cdot FPR \; + \; \rp \cdot cost_{FN} \cdot FNR}{(1 - \rp) \cdot cost_{FP} \; + \; \rp \cdot cost_{FN}} \in [0,1]$$
\begin{align*}
Costs_{norm}(\rp)
&= \tfrac{(1 - \rp) \cdot FPR \cdot cost_{FP}  \; + \; \rp \cdot FNR \cdot cost_{FN} }{(1 - \rp) \cdot cost_{FP} \; + \; \rp \cdot cost_{FN}}\\
&= {\color{red}\tfrac{(1 - \rp)\cdot cost_{FP} \cdot {\color{black}FPR}}{(1 - \rp) \cdot cost_{FP} \; + \; \rp \cdot cost_{FN}}} + {\color{blue}\tfrac{\rp \cdot cost_{FN} \cdot {\color{black}FNR}}{(1 - \rp) \cdot cost_{FP} \; + \; \rp \cdot cost_{FN}}}
\end{align*}

%Let $PC(+)$ be the normalized version of $\rp\cdot cost_{FN}$, where $PC$ stands for "probability times cost", we have
Let "probability times cost" $PC(+)$ be normalized version of $\rp\cdot cost_{FN}$:
\begin{align*}
   {\color{blue}PC(+)=\tfrac{\rp\cdot cost_{FN}}{(1-\rp)\cdot cost_{FP} + \rp\cdot cost_{FN}}} \text{ and }
   {\color{red}1-PC(+)=\tfrac{(1-\rp)\cdot cost_{FP}}{(1-\rp)\cdot cost_{FP} + \rp\cdot cost_{FN}}}
\end{align*}
To obtain cost lines, we need a function with slope $(FNR - FPR)$ and intercept $FPR$ $\Rightarrow$ Rewrite $Costs_{norm}(\rp)$ as function of $PC(+)$:
\begin{align*}
Costs_{norm}(PC(+)) &= (1-PC(+))\cdot FPR + PC(+)\cdot FNR \\
&= (FNR - FPR)\cdot PC(+) + FPR \\
&= \begin{cases}
  FPR, if\; PC(+) = 0 \\
  FNR, if\; PC(+) = 1
\end{cases}
\end{align*}
%\vspace{-0.8cm}
\begin{columns}[c]
\begin{column}{0.49\textwidth}
\begin{itemize}
  \item Plot is similar to simplified case with $cost_{FN} = cost_{FP}$ %in comparison with the simpler setting's.
  \item Axes' labels and their interpretation have changed
  \item Normalized cost vs. "probability times cost"
  %range of class distributions and misclassifcation costs.
\end{itemize}
\end{column}
\begin{column}{0.49\textwidth}
\begin{figure}
  \centering\includegraphics[width=0.75\textwidth]{figure/cost_curve.png}
\end{figure}
\end{column}
\end{columns}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{compare with trivial classifiers}
  \begin{itemize}
    \item Operating range of a classifier is a set of $PC(+)$ values (operating points) where classifier performs better than both trivial classifiers
    \item Intersection of cost curves and trivial classifiers' diagonals determine operating range
    %this range is defined by two $PC(+)$ values, which are the intersections between the classifier's lower envelope (cost curve) and the trivial classifiers' diagonals.
    \item At any $PC(+)$ value, the vertical distance of trivial diagonal to a classifer's cost curve within operating range shows advantage in performance (normalized costs) of classifier %provides a quantitative performance advantage of that classifier over trivial classifier at that operating point.
    %The vertical distance between a trivial diagonal and the classifer's cost curve at any $PC(+)$ value within the operating range provides a quantitative performance advantage of that classifier over trivial classifier at that operating point.
  \end{itemize}
  %\pagebreak
  %In the plot below,
\begin{columns}[T]
\begin{column}{0.6\textwidth}
\textbf{Example:} Dotted lines are operating range of a classifier (here: $[0.14, 0.85]$)
\end{column}
\begin{column}{0.39\textwidth}
    \includegraphics[width=\textwidth]
      {figure/cost_curve_compare_trivial.png}
    % \tiny{\\ Credit: Chris Drummond and Robert C. Holte  \\}
\end{column}
\end{columns}


\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Comparing classifiers}
  \begin{itemize}
    \item If classifier C1's expected cost is lower than classifier C2's at a $PC(+)$ value, C1 outperforms C2 at that operating point
    \item The two cost curves of C1 and C2 may cross, which indicates C1 outperforms C2 for a certain operating range and vice versa
    \item The vertical distance between the two cost curves of C1 and C2 at any $PC(+)$ value directly indicates the performance difference between them at that operating point
  \end{itemize}


\begin{columns}[T]
\begin{column}{0.6\textwidth}
\textbf{Example:} Dotted cost curve has lower expected cost as dashed cost curve for $PC(+) < 0.5$ and hence outperforms dashed one in this operating range and vice versa
\end{column}
\begin{column}{0.39\textwidth}
\includegraphics[width=0.9\textwidth]{figure_man/cost-curves-classifiers-comparison.png}
    \tiny
    \\Chris Drummond and Robert C. Holte (2006): \\
    Cost curves: An improved method for visualizing classifier performance.
    Machine Learning, 65, 95-130
    (\href{https://www.semanticscholar.org/paper/Cost-curves\%3A-An-improved-method-for  -visualizing-Drummond-Holte/71708ce984e0896e7383435913547e770572410e}
    {\underline{URL}})
\end{column}
\end{columns}

  %
  % \begin{figure}
  %   \centering
  %   \includegraphics[width=0.5\textwidth]
  %     {figure_man/cost-curves-classifiers-comparison.png}
  %   \tiny
  %   \\Chris Drummond and Robert C. Holte (2006): \\
  %   Cost curves: An improved method for visualizing classifier performance.
  %   \\Machine Learning, 65, 95-130
  %   (\href{https://www.semanticscholar.org/paper/Cost-curves\%3A-An-improved-method-for  -visualizing-Drummond-Holte/71708ce984e0896e7383435913547e770572410e}
  %   {\underline{URL}}).
  %   % \tiny{\\ Credit: Chris Drummond and Robert C. Holte  \\}
  % \end{figure}

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{ROC curves vs. cost curves}

\begin{itemize}
  \item A point/line in ROC space is represented by a line/point in cost space, and vice versa
  \item Area under an ROC curve is a ranking measure while area under a cost curve is the expected cost of the classifier (assuming that all possible $PC(+)$ values are equally likely)
  \item ROC curves do not indicate for which prob threshold classifier A is superior to another classifier B, cost curves can do exactly that!\\
  $\Rightarrow$ Cost curves practically more useful than ROC curves
  %therefore provide practically more relevant information than ROC curves.
  \item Cost curves allows users to measure quantitative performance difference between multiple classifiers at any given operating point \\
  $\Rightarrow$ Not so easy to do that with ROC curve
  %($\rightarrow$ only the axes will change to account for the costs).
  % ($\rightarrow$ simple modification where the identity of the axes is changed).
  %\item Then, the y-axis represents the normalized expected cost (NEC) or relative expected misclassification cost.
  \end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{ROC curves vs. cost curves}
% The general form is as follows:
%
% \vspace{-0.5cm}
% \begin{center}
% \begin{equation*}
% NEC = \text{FNR} \cdot P_C[+] + \text{FPR} \cdot (1 - P_C[+]),
% \end{equation*}
% \end{center}
%
% \begin{itemize}
% \item FNR/FPR are false-negative rate and false-positive rate respectively and
% $P_C[+]$, the probability cost function (modified version of $P[+]$ that takes costs into
% consideration).
%
% \vspace{-0.5cm}
% \begin{center}
% \begin{equation*}
% P_C[+] = \frac{P[+] \cdot C[+|-]}{P[+] \cdot C[+|-] + P[-] \cdot C[-|+]},
% \end{equation*}
% \end{center}
%
% \item $C[+|-]$ and $C[-|+]$ represent the cost of predicting a positive when the
% instance is actually negative and vice versa and $P[-]$ as the probability of
% being in the negative class.
% \end{itemize}
% \end{vbframe}

% ------------------------------------------------------------------------------
\endlecture
\end{document}
