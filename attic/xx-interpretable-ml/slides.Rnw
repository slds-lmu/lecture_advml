<<setup-child, include = FALSE>>=
library(knitr)
library(mlr)
library(partykit)
library(vcd)
library(iml)
library(gridExtra)


set_parent("../style/preamble.Rnw")

set.seed(123)
load("bike.RData")
task = makeRegrTask(data = bike, target = "cnt")
mod = train("regr.randomForest", task)
predictor = Predictor$new(mod, data = bike[-which(names(bike) == "cnt")], y = bike$cnt)
bike.x = bike[names(bike) != 'cnt']
@

\lecturechapter{17}{Interpretable Machine Learning}
\lecture{Fortgeschrittene Computerintensive Methoden}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\begin{vbframe}{INTERPRETABLE MACHINE LEARNING}
 \begin{center}
  \vspace{-0.5cm}
  \includegraphics[width=0.62\textwidth]{figure_man/black-box.png}
 \end{center}

 \begin{itemize}
  \item Machine learning (ML) has huge potential to improve research, products and processes
  \item Powerful ML models usually are intransparent black or rather greyish boxes:
    e.g. bagged or boosted trees, RBF SVM, deep networks
  \item The lack of explanation hurts trust and creates barriers for their adoption
 \end{itemize}

%\pagebreak
%
% \begin{center}
%  \includegraphics[width=0.72\textwidth]{figure_man/trend}
% \end{center}
%
% \begin{center}
%  \includegraphics[width=0.48\textwidth]{figure_man/wordcloud}
% \end{center}
% \tiny{Adadi, Amina, and Mohammed Berrada. "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)." IEEE Access (2018).}
\end{vbframe}


\begin{vbframe}{When do we need interpretability?}
\begin{columns}
\begin{column}{0.6\textwidth}
 \begin{itemize}
  \item Debugging machine learning models
  \item Increasing trust in models
  \item Analyzing newly developed systems with unknown consequences
  \item Decisions about humans
  \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
  \item When loss function doesn't cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
\end{itemize}
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
 \begin{center}
  \includegraphics[width=0.7\textwidth]{figure_man/explain-to}
 \end{center}
\end{column}
\end{columns}
 \lz
\tiny{Doshi-Velez, F., and Kim, B. (2017)}
\tiny{Adadi, Amina, and Mohammed Berrada (2018)}
\end{vbframe}


\begin{vbframe}{What tools do we have?}
 \begin{center}
  \includegraphics[width=0.9\textwidth]{figure_man/overview}
 \end{center}
\end{vbframe}

\begin{vbframe}{Intrinsic and Post-Hoc Interpretation}
\begin{itemize}
  \item Intrinsically interpretable models
  \begin{itemize}
  \item Models such as linear models and decision trees
  \item Interpretable because of simple structures, e.g.,
    weighted combination of feature values or tree structure
  \item Become difficult to interpret with many features or complex interaction terms
  \end{itemize}
  \item Post-hoc interpretation methods
  \begin{itemize}
  \item Are applied after training
  \item Also work for more complex black box models
  \item Can also be applied to intrinsically interpretable models, e.g.,
    feature importance for decision trees
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Model-Agnostic Interpretability}
 \begin{itemize}
  \item Model-agnostic interpretability methods should work for \textbf{any} kind of machine learning model
  \item Orthogonal combination: Explanation type not tied to underlying model type
  \item Often, only access to data and fitted predictor is required
  \item No further knowledge about the model itself required
 \end{itemize}
\end{vbframe}


\begin{vbframe}{Global and Local Interpretability}
Global interpretability methods explain the model behavior for the entire input space. For example:
  \begin{itemize}
    \item Permutation Feature Importance
    \item Partial Dependence Plot
    \item Feature Interaction (H-Statistic)
    \item Global Surrogate Models
  \end{itemize}
\lz
Local interpretability methods explain single predictions. For example:
 \begin{itemize}
  \item Individual Conditional Expectation
  \item LIME
  \item Shapley Value
  \item Counterfactual explanations
 \end{itemize}
\end{vbframe}

\begin{vbframe}{Fixed model vs. refits}
  \begin{itemize}
     \item Most methods presented in this lecture analyze a fixed, trained model (e.g. permutation feature importance)
     \item Some methods require refitting the model (e.g. PIMP)
     \item Trained model $\Rightarrow$ Model is the object of analysis
     \item Refitting $\Rightarrow$ Learning process is the object of analysis
     \item Advantage of refitting: Information about variability in learning process
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Effect and Importance}
\textbf{Feature Importance} scores features by how much they contribute to the predictive performance or prediction variance of the model.
  \begin{itemize}
    \item Methods: Permutation Feature Importance, Functional Anova
    \item  Analog in linear model: Absolute t-statistic $\left|\frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}\right|$
  \end{itemize}

\lz

\textbf{Feature Effects} visualize or quantify the (average) relationship between the features and the predictions.
  \begin{itemize}
    \item Methods: Partial Dependence Plots, Individual Conditional Expectation, Accumulated Local Effects
    \item Analog in linear model: Regression coefficient $\hat{\theta}_j$
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Data and Model for Examples}
Hourly and daily count of rental bikes between years 2011 and 2012 from Capital Bikeshare system with the corresponding weather and seasonal information.\\
 \lz
We fit a random forest to predict daily number of rented bikes given weather and seasonal data.\\
 \lz
% \url{https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset}
\url{https://www.openml.org/d/41979}
\end{vbframe}


\begin{vbframe}{Data and Model for Examples}
Feature information for the bike sharing data set.\\
<<bike-plot1, echo=FALSE, message=FALSE, warning=FALSE, size="small">>=
print(mlr::summarizeColumns(bike)[, -c(3, 5:9)])
@
\end{vbframe}


\begin{vbframe}{Comparison of Two Models}
Why not use an interpretable linear model? We compare the mean absolute error of a linear model to a random forest via 10-fold CV.\\
 \lz
<<bike-plot2, echo=FALSE, message=FALSE, warning=FALSE, size="tiny">>=
set.seed(123)
rdesc = makeResampleDesc("CV",iter = 10)
regr.randomForest.cv.mae = resample("regr.randomForest", task, rdesc, measures = mae, show.info = FALSE)
# regr.randomForest.cv.mae
regr.lm.cv.mae = resample("regr.lm", task, rdesc, measures = mae, show.info = FALSE)
# regr.lm.cv.mae
@
\begin{center}
\begin{tabular}{ |c|c| }
\hline
Model & MAE (10-fold CV) \\
\hline
random forest &  \Sexpr{regr.randomForest.cv.mae$aggr}\\
linear model &  \Sexpr{regr.lm.cv.mae$aggr}\\
\hline
\end{tabular}
\end{center}
\lz
$\to$We select the random forest.
\end{vbframe}




%% Permutation Feature Importance
\begin{vbframe}{Permutation Feature Importance}

One possible definition: Feature importance is the increase in the model's test loss after permuting the feature's values.

<<bike-plot3, echo=FALSE, message = FALSE, warning = FALSE, out.width = "75%">>=
set.seed(123)
importance = FeatureImp$new(predictor, loss = 'mse')
plot(importance)
@
\tiny{Fisher, A., Rudin, C., and Dominici, F. (2018). Model Class Reliance.}

\normalsize
\pagebreak

1. Estimate model's prediction error on test data\\
2. For each feature $x_j$
\begin{itemize}
 \item Shuffle the feature
\end{itemize}
	\vspace{-0.7cm}
\begin{columns}
\begin{column}{0.4\textwidth}
\center{original} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 & 1.4 & Apple & 0.2\\
\hline
9.1 & 1.2 & Mango & 0.3\\
\hline
4.0 & 1.8 & Lychee & 0.1\\
\hline
6.2 & 1.0 & Melon & 0.3\\
\hline
10.0 & 7.1 & Strawberry & 0.9 \\
\hline
8.6 & 3.0 & Lemon & 0.4\\
\hline
9.4 & 7.0 & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}

\begin{column}{0.2\textwidth}
\begin{center}
\vspace{1.2cm}
$\Rightarrow$
\end{center}
\end{column}

\begin{column}{0.4\textwidth}
\center{shuffled $x_2$} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 &  7.1 & Apple & 0.2\\
\hline
9.1 & 3.0& Mango & 0.3\\
\hline
4.0 & 1.8 & Lychee & 0.1\\
\hline
6.2 & 1.4 & Melon & 0.3\\
\hline
10.0 &1.0& Strawberry & 0.9 \\
\hline
8.6 &  7.0 & Lemon & 0.4\\
\hline
9.4 & 1.2  & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}
\end{columns}

\lz

\begin{itemize}
 \item Estimate the prediction error of the model on test data after shuffling
 \item Calculate importance as factor of increase in model's prediction error (
 \item Average the feature importance over multiple repetitions
\end{itemize}
\end{vbframe}



\begin{vbframe}{Permutation Feature Importance}
\addtocounter{framenumber}{-1}
\begin{columns}
\begin{column}{0.4\textwidth}
\center{original} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 & 1.4 & Apple & 0.2\\
\hline
9.1 & 1.2 & Mango & 0.3\\
\hline
4.0 & 1.8 & Lychee & 0.1\\
\hline
6.2 & 1.0 & Melon & 0.3\\
\hline
10.0 & 7.1 & Strawberry & 0.9 \\
\hline
8.6 & 3.0 & Lemon & 0.4\\
\hline
9.4 & 7.0 & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}

\begin{column}{0.2\textwidth}
\begin{center}
\vspace{1.5cm}
$\Rightarrow$
\end{center}
\end{column}


\begin{column}{0.4\textwidth}
\center{shuffled $x_1$} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
8.6 & 1.4 & Apple & 0.2\\
\hline
4.0 & 1.2 & Mango & 0.3\\
\hline
10.0 & 1.8 & Lychee & 0.1\\
\hline
9.1 & 1.0 & Melon & 0.3\\
\hline
6.2 & 7.1 & Strawberry & 0.9 \\
\hline
9.4 & 3.0 & Lemon & 0.4\\
\hline
7.0 & 7.0 & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}
\end{columns}

\end{vbframe}
\begin{vbframe}{Permutation Feature Importance}
\addtocounter{framenumber}{-1}
\begin{columns}
\begin{column}{0.4\textwidth}
\center{original} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 & 1.4 & Apple & 0.2\\
\hline
9.1 & 1.2 & Mango & 0.3\\
\hline
4.0 & 1.8 & Lychee & 0.1\\
\hline
6.2 & 1.0 & Melon & 0.3\\
\hline
10.0 & 7.1 & Strawberry & 0.9 \\
\hline
8.6 & 3.0 & Lemon & 0.4\\
\hline
9.4 & 7.0 & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}

\begin{column}{0.2\textwidth}
\begin{center}
\vspace{1.2cm}
$\Rightarrow$
\end{center}
\end{column}

\begin{column}{0.4\textwidth}
\center{shuffled $x_2$} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 &  7.1 & Apple & 0.2\\
\hline
9.1 & 3.0& Mango & 0.3\\
\hline
4.0 & 1.8 & Lychee & 0.1\\
\hline
6.2 & 1.4 & Melon & 0.3\\
\hline
10.0 &1.0& Strawberry & 0.9 \\
\hline
8.6 &  7.0 & Lemon & 0.4\\
\hline
9.4 & 1.2  & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Permutation Feature Importance}
\addtocounter{framenumber}{-1}
\begin{columns}
\begin{column}{0.4\textwidth}
\center{original} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 & 1.4 & Apple & 0.2\\
\hline
9.1 & 1.2 & Mango & 0.3\\
\hline
4.0 & 1.8 & Lychee & 0.1\\
\hline
6.2 & 1.0 & Melon & 0.3\\
\hline
10.0 & 7.1 & Strawberry & 0.9 \\
\hline
8.6 & 3.0 & Lemon & 0.4\\
\hline
9.4 & 7.0 & Kiwifruit & 0.1\\
\hline
\end{tabular}
}
\end{column}

\begin{column}{0.2\textwidth}
\begin{center}
\vspace{1.2cm}
$\Rightarrow$
\end{center}
\end{column}

\begin{column}{0.4\textwidth}
\center{shuffled $x_3$} \\
\scalebox{0.9}{
\small
\begin{tabular}{|l | l | l | l | }
\hline
$x_1$ & $x_2$ & $x_3$  &$y$\\
\hline
7.0 &  1.4 & Kiwifruit  & 0.2\\
\hline
9.1 & 1.2 & Strawberry& 0.3\\
\hline
4.0 & 1.8 & Lemon  & 0.1\\
\hline
6.2 & 1.0 & Melon & 0.3\\
\hline
10.0 & 7.1 & Lychee & 0.9 \\
\hline
8.6 &  3.0 & Mango  & 0.4\\
\hline
9.4 & 7.0 & Apple & 0.1\\
\hline
\end{tabular}
}
\end{column}
\end{columns}



\pagebreak

<<bike-plot4, echo=FALSE, message = FALSE, warning = FALSE, out.width = "65%">>=
set.seed(123)
importance = FeatureImp$new(predictor, loss = 'mse')
plot(importance)
@

\begin{itemize}
 \item The year is the most important feature
 \item Interpretation of year: Destroying the information about the year by shuffling increases MSE by a factor of 18
\end{itemize}
\end{vbframe}


\begin{vbframe}{Permutation Feature Importance}
 \begin{itemize}
  \item Interpretation: Feature importance is the increase of model error when the feature's information is destroyed
  \lz
  \item Results can be unstable, because the importance calculation relies on permutations\\
  \lz
$\Rightarrow$ Average results over multiple repetitions
  \lz
  \item Features are permuted regardless of correlation with other features\\
  \lz
$\Rightarrow$ Can result in unrealistic combination of feature values
  \lz
  \item Feature importance automatically includes importance of interaction effects with other features
 \end{itemize}
\end{vbframe}


\begin{vbframe}{Testing Importance (PIMP)}

\begin{itemize}
  \item PIMP was introduced for random forest feature importance
  \item Fixes problem that importance measure prefers features with many categories
  \item Also: When is the importance significantly different from 0?
  \item PIMP computes the distribution of importances under the $H_0$-hypothesis that the feature has no influence
  \item Sampling under $H_0$ achieved by permuting $\ydat$-vector and model retraining
  \item We now rescale the importance to a p-value - the tail probability under $H_0$ - as new importance score
\end{itemize}

{\tiny{Altmann, Andr√©, et al. "Permutation importance: a corrected feature importance measure." Bioinformatics 26.10 (2010): 1340-1347.}}

\pagebreak

PIMP algorithm:

\begin{itemize}
	\item For $m \in \{1, \ldots, n_{repetitions}\}$:
		\begin{itemize}
			\item Permute response vector $\ydat$
			\item Retrain model with data $\Xmat$ and permuted $\ydat$
			\item Compute feature importance $FI_j^m$ for each feature $j$
		\end{itemize}
	\item Train model with $\Xmat$ and unpermuted $\ydat$
	\item For each feature $j \in \{1,\ldots,p\}$
		\begin{itemize}
			\item Fit probability distribution of the feature importance values $FI_j^m$, $m \in \{1, \ldots, n_{repetitions}\}$ (choice between Gaussian, lognormal, gamma or non-parametric)
			\item Compute permutation feature importance $FI_j$ for model without permutation
			\item Retrieve p-value of $FI_j$ based on fitted distribution
		\end{itemize}
\end{itemize}

\end{vbframe}


%%pdp

\begin{vbframe}{ICE and Partial Dependence}
The individual conditional expectation (ICE) shows the effect of feature(s) $x_S$ on the model prediction for a single observation, conditional on the vector of complementary features $x_C$.

$$f_{S}^{(i)}(x_S) = f(x_S, x_C^{(i)})$$

The partial dependence (PD) is the expectation of the ICE with respect to the marginal distribution of complementary features $x_C$, i.e., the ICE if the values of all complementary features $x_C$ are set to the expected value of their joint distribution.

$$
f_{S}(x_S) = \E_{x_C} f(x_S, x_C) = \int_{-\infty}^{\infty} f(x_S, x_C) \, d\mathcal{P}(x_C)
$$

In practice, $x_S$ consists of one or two features.\\

<<pdp-1, echo=FALSE, fig.height=4, message=FALSE, warning=FALSE, out.height = "45%", eval=FALSE>>=
set.seed(123)
pred.bike = Predictor$new(mod, data = bike)
pdp = FeatureEffect$new(pred.bike, "temp", method = "pdp")
p1 = pdp$plot() + scale_x_continuous('Temperature', limits = c(0, NA)) + scale_y_continuous('Predicted number of bike rentals', limits = c(0, 5500))
pdp$set.feature("hum")
p2 = pdp$plot() +  scale_x_continuous('Humidity', limits = c(0, NA)) + scale_y_continuous('', limits = c(0, 5500))
pdp$set.feature("windspeed")
p3 = pdp$plot() + scale_x_continuous('Windspeed', limits = c(0, NA)) + scale_y_continuous('', limits = c(0, 5500))
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
@
  \lz
\tiny{Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2013). Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation, 1-22. https://doi.org/10.1080/10618600.2014.907095}

\tiny{Friedman, J.H. 2001. "Greedy Function Approximation: A Gradient Boosting Machine." Annals of Statistics 29: 1189-1232.}

  \normalsize
  \pagebreak

%Instead of a grid over a single feature, the PDP can also be plotted as a heatmap for two features.
  \lz
<<pdp-2, echo=FALSE, fig.height=4.2, message=FALSE, warning=FALSE, eval=FALSE>>=
set.seed(123)
pred.bike = Predictor$new(mod, data = bike)
pdp.2feature = FeatureEffect$new(pred.bike, feature = c("temp", "hum"), method = "pdp")
pdp.2feature$plot() + scale_x_continuous('Temperature', limits = c(0, NA)) +  scale_y_continuous('Humidity', limits = c(0, NA))
@


\pagebreak

Algorithm for calculating the ICE curves for a single feature $x_j$ (i.e. $x_j$ is the only feature in $x_S$):
\begin{itemize}
  \item Select a feature $x_j$
  \item Choose grid points along $x_j$
  \item For each grid point:
    \begin{itemize}
      \item Overwrite feature $x_j$ in the dataset with the current grid value\\
      \item Get the predictions for these points from the ML model\\
      \item Draw a curve with the grid points on the x-axis and the average prediction on the y-axis
    \end{itemize}
\end{itemize}

Algorithm for the partial dependence plot (PDP):
	\begin{itemize}
		\item Average the predictions per grid point
		\item Draw a curve with the grid points on the x-axis and the average prediction on the y-axis
\end{itemize}
\end{vbframe}


\begin{frame}{ICE}
<<pdp-3, echo=FALSE, fig.height=4.2, message=FALSE, warning=FALSE>>=
  i = pch = rep(1:3, 3) #c(2,3,1,3,1,2)
x = c(1,1,1,2,2,2,3,3,3)
y = c(
  1,1,0,
  1,1,0,
  1,1,0)
fh = c(
  0.7, 0.7, 0.1,
  0.4, 0.7, 0.1,
  0.7, 0.6, 0.3)
l = abs(y - fh)
li = l[c(1,5,9)]
li = li[i]
dL = c(0.4,0.6,0.1,0.6,0.8,0.5,0.7,0.9,0.6)
lreplace = dL + mean(l)
X = data.frame(pch, x, dL)
X.aggr = aggregate(dL ~ x, data = X, FUN = mean, na.rm = TRUE)
@

  \begin{onlyenv}<1>
  \vspace{1cm}
    \begin{columns}[T]
    \begin{column}{0.3\textwidth}
    \centering
    \scalebox{.75}{
      \begin{tabular}{|l|l|ll|c|}
      \hline
      i & $ x_1$ & $ x_2$ & $ x_3$ & $\hat{f}^{(i)}_{S}$ \\
      \hline
      1 & \textbf{1} & 2 & 3 & \textbf{\Sexpr{X$dL[1]}}  \\
      2 & \textbf{1} & 4 & 5 & \textbf{\Sexpr{X$dL[2]}}  \\
      3 & \textbf{1} & 6 & 7 & \textbf{\Sexpr{X$dL[3]}} \\
      \hline
      1 & 2 & 2 & 3 &  $\Sexpr{X$dL[4]}$  \\
      2 & 2 & 4 & 5 &  $\Sexpr{X$dL[5]}$  \\
      3 & 2 & 6 & 7 &  $\Sexpr{X$dL[6]}$  \\
      \hline
      1 & 3 & 2 & 3 &  $\Sexpr{X$dL[7]}$  \\
      2 & 3 & 4 & 5 &  $\Sexpr{X$dL[8]}$  \\
      3 & 3 & 6 & 7 &  $\Sexpr{X$dL[9]}$  \\
      \hline
      \end{tabular}
    }
    \end{column}


    \begin{column}{0.7\textwidth}
    <<pdp-4, echo=FALSE, results="hide", message=FALSE, eval = TRUE>>=
    par(mar = c(3,3.5,0.25,0.25))
    pch.sym = paste0("i=", c("1","2","3"))
    p = pch.sym[pch]
    #p[p == x] = NA

    plot(x,dL, pch = p, ylim = c(0,1), col = "white", las = 2, xlab = "", ylab = "", xaxt = "n", yaxt = "n")
    title(xlab = expression(x[1]), line = 2)
    title(ylab = expression(hat(f)[S]), line = 2)
    axis(1, at = c(1,2,3), padj = -0.5)
    axis(2, at = seq(0, 1, by = 0.25), labels = c("0", "0.25", "0.5", "0.75", "1"), padj = 0.5)
    grid()
    lapply(split(X, X$pch), function(d) lines(d$x, d$dL, lty = d$pch + 1, lwd = 2))
    #lines(X.aggr$x, X.aggr$dL, lwd = 2, col = "red")
    shadowtext = function(x, y = NULL, labels, col = 'black', bg = 'white',
                          theta = seq(0, 2*pi, length.out = 50), r = 0.1, ... ) {

      xy = xy.coords(x,y)
      xo = r*strwidth('A')
      yo = r*strheight('A')

      # draw background text with small shift in x and y in background colour
      for (i in theta) {
        text(xy$x + cos(i)*xo, xy$y + sin(i)*yo, labels, col = bg, ... )
      }
      # draw actual text in exact xy position in foreground colour
      text(xy$x, xy$y, labels, col = col, ... )
    }
    points(x, dL, col = c(rep(rgb(0,155,165, max = 255), 3), rep("black", 6)), pch = 19)
    shadowtext(x, dL - 0.03, labels = p, col = c(rep(rgb(0,150,160, max = 255), 3), rep("black", 6)))
    @
      \end{column}
    \end{columns}
  \end{onlyenv}


\begin{onlyenv}<2>
\vspace{1cm}
\begin{columns}[T]
\begin{column}{0.3\textwidth}
\centering
\scalebox{.75}{
\begin{tabular}{|l|l|ll|c|}
\hline
i & $ x_1$ & $ x_2$ & $ x_3$ & $\hat{f}^{(i)}_{S}$ \\
\hline
1 &  1 & 2 & 3 &  $\Sexpr{X$dL[1]}$  \\
2 &  1 & 4 & 5 &  $\Sexpr{X$dL[2]}$  \\
3 &  1 & 6 & 7 &  $\Sexpr{X$dL[3]}$ \\
\hline
1 & \textbf{2} & 2 & 3 &  \textbf{\Sexpr{X$dL[4]}}  \\
2 & \textbf{2} & 4 & 5 &  \textbf{\Sexpr{X$dL[5]}}  \\
3 & \textbf{2} & 6 & 7 &  \textbf{\Sexpr{X$dL[6]}}  \\
\hline
1 & 3 & 2 & 3 &  $\Sexpr{X$dL[7]}$  \\
2 & 3 & 4 & 5 &  $\Sexpr{X$dL[8]}$  \\
3 & 3 & 6 & 7 &  $\Sexpr{X$dL[9]}$  \\
\hline
\end{tabular}
}
\end{column}
\begin{column}{0.7\textwidth}
<<pdp-5, echo=FALSE, results="hide", message=FALSE, eval = TRUE>>=
par(mar = c(3,3.5,0.25,0.25))
pch.sym = paste0("i=", c("1","2","3"))
p = pch.sym[pch]
#p[p == x] = NA
plot(x,dL, pch = p, ylim = c(0,1), col = "white", las = 2, xlab = "", ylab = "", xaxt = "n", yaxt = "n")
title(xlab = expression(x[1]), line = 2)
title(ylab = expression(hat(f)[S]), line = 2)
axis(1, at = c(1,2,3), padj = -0.5)
axis(2, at = seq(0, 1, by = 0.25), labels = c("0", "0.25", "0.5", "0.75", "1"), padj = 0.5)
grid()
lapply(split(X, X$pch), function(d) lines(d$x, d$dL, lty = d$pch + 1, lwd = 2))
#lines(X.aggr$x, X.aggr$dL, lwd = 2, col = "red")
shadowtext = function(x, y = NULL, labels, col = 'black', bg = 'white',
  theta = seq(0, 2*pi, length.out = 50), r = 0.1, ... ) {

  xy = xy.coords(x,y)
  xo = r*strwidth('A')
  yo = r*strheight('A')

  # draw background text with small shift in x and y in background colour
  for (i in theta) {
    text(xy$x + cos(i)*xo, xy$y + sin(i)*yo, labels, col = bg, ... )
  }
  # draw actual text in exact xy position in foreground colour
  text(xy$x, xy$y, labels, col = col, ... )
}
points(x, dL, col = c(rep("black", 3), rep(rgb(0,150,160, max = 255), 3), rep("black", 3)), pch = 19)
shadowtext(x, dL - 0.03, labels = p, col = c(rep("black", 3), rep(rgb(0,150,160, max = 255), 3), rep("black", 3)))
@
\end{column}
\end{columns}
\end{onlyenv}


\begin{onlyenv}<3>
\vspace{1cm}
\begin{columns}[T]
\begin{column}{0.3\textwidth}
\centering
\scalebox{.75}{
\begin{tabular}{|l|l|ll|c|}
\hline
i & $ x_1$ & $ x_2$ & $ x_3$ & $\hat{f}^{(i)}_{S}$ \\
\hline
1 &  1 & 2 & 3 & $\Sexpr{X$dL[1]}$  \\
2 &  1 & 4 & 5 & $\Sexpr{X$dL[2]}$  \\
3 &  1 & 6 & 7 & $\Sexpr{X$dL[3]}$ \\
\hline
1 & 2 & 2 & 3 &  $\Sexpr{X$dL[4]}$  \\
2 & 2 & 4 & 5 &  $\Sexpr{X$dL[5]}$  \\
3 & 2 & 6 & 7 &  $\Sexpr{X$dL[6]}$  \\
\hline
1 & \textbf{3} & 2 & 3 &  \textbf{\Sexpr{X$dL[7]}} \\
2 & \textbf{3} & 4 & 5 &  \textbf{\Sexpr{X$dL[8]}}  \\
3 & \textbf{3} & 6 & 7 &  \textbf{\Sexpr{X$dL[9]}}  \\
\hline
\end{tabular}
}
\end{column}
\begin{column}{0.7\textwidth}  %%<--- here
<<pdp-6, echo = FALSE, results = "hide", message = FALSE, eval = TRUE>>=
par(mar = c(3,3.5,0.25,0.25))
pch.sym = paste0("i=", c("1","2","3"))
p = pch.sym[pch]
#p[p == x] = NA

plot(x,dL, pch = p, ylim = c(0,1), col = "white", las = 2, xlab = "", ylab = "", xaxt = "n", yaxt = "n")
title(xlab = expression(x[1]), line = 2)
title(ylab = expression(hat(f)[S]), line = 2)
axis(1, at = c(1,2,3), padj = -0.5)
axis(2, at = seq(0, 1, by = 0.25), labels = c("0", "0.25", "0.5", "0.75", "1"), padj = 0.5)
grid()
lapply(split(X, X$pch), function(d) lines(d$x, d$dL, lty = d$pch + 1, lwd = 2))
#lines(X.aggr$x, X.aggr$dL, lwd = 2, col = "red")
shadowtext = function(x, y = NULL, labels, col = 'black', bg = 'white',
  theta = seq(0, 2*pi, length.out = 50), r = 0.1, ... ) {

  xy = xy.coords(x,y)
  xo = r*strwidth('A')
  yo = r*strheight('A')

  # draw background text with small shift in x and y in background colour
  for (i in theta) {
    text(xy$x + cos(i)*xo, xy$y + sin(i)*yo, labels, col = bg, ... )
  }
  # draw actual text in exact xy position in foreground colour
  text(xy$x, xy$y, labels, col = col, ... )
}
points(x, dL, col = c(rep("black", 6), rep(rgb(0,150,160, max = 255), 3)), pch = 19)
shadowtext(x, dL - 0.03, labels = p, col = c(rep("black", 6), rep(rgb(0,150,160, max = 255), 3)))
@
\end{column}
\end{columns}
\end{onlyenv}
\end{frame}



\begin{vbframe}{Partial Dependence Plot}

\vspace{1cm}
\begin{columns}[T]
\begin{column}{0.3\textwidth}
\centering
\scalebox{.75}{
  \begin{tabular}{|l|ll|c|}
  \hline
  $ x_1$ & $ x_2$ & $ x_3$ & $\hat{f}_S$ \\
  \hline
  1 &  2 &  3 &   $\frac{1}{3} \sum_{i=1}^3\hat{f}_S^{(i)} (1)$ \\
  2 &  4 &  5 &   $\frac{1}{3} \sum_{i=1}^3\hat{f}_S^{(i)} (2)$ \\
  3 &  6 &  7 &   $\frac{1}{3} \sum_{i=1}^3\hat{f}_S^{(i)} (3)$ \\
  \hline
  \end{tabular}
}
\end{column}
\begin{column}{0.7\textwidth}  %%<--- here
<<pdp-7, echo=FALSE, results = "hide", message=FALSE, eval = TRUE>>=
  par(mar = c(3,3.5,0.25,0.25))
pch.sym = paste0("i=", c("1","2","3"))
p = pch.sym[pch]
#p[p == x] = NA

plot(x,dL, pch = p, ylim = c(0,1), col = "white", las = 2, xlab = "", ylab = "", xaxt = "n", yaxt = "n")
title(xlab = expression(x[1]), line = 2)
title(ylab = expression(hat(f)[S]), line = 2)
axis(1, at = c(1,2,3), padj = -0.5)
axis(2, at = seq(0, 1, by = 0.25), labels = c("0", "0.25", "0.5", "0.75", "1"), padj = 0.5)
grid()
lapply(split(X, X$pch), function(d) lines(d$x, d$dL, lty = d$pch + 1, lwd = 2))
lines(X.aggr$x, X.aggr$dL, lwd = 3, col = rgb(0, 150, 160, max = 255))
shadowtext = function(x, y = NULL, labels, col = 'black', bg = 'white',
                      theta = seq(0, 2*pi, length.out = 50), r = 0.1, ... ) {

  xy = xy.coords(x,y)
  xo = r*strwidth('A')
  yo = r*strheight('A')

  # draw background text with small shift in x and y in background colour
  for (i in theta) {
    text(xy$x + cos(i)*xo, xy$y + sin(i)*yo, labels, col = bg, ... )
  }
  # draw actual text in exact xy position in foreground colour
  text(xy$x, xy$y, labels, col = col, ... )
}
points(x, dL, col = "black", pch = 19)
shadowtext(x, dL - 0.03, labels = p, col = "black")
@

  \end{column}
\end{columns}
\end{vbframe}


\begin{vbframe}{Example: Partial Dependence Plots}

<<pdp-8, echo=FALSE, message = FALSE, out.width = "50%", eval = TRUE>>=

set.seed(123)
pred.bike = Predictor$new(mod, data = bike)
pdp = FeatureEffect$new(pred.bike, "hum", method = "pdp")
p2 = pdp$plot() +  scale_x_continuous('Humidity', limits = c(0, NA)) + scale_y_continuous('', limits = c(0, 5500))

p2
@

\begin{itemize}
 \item Relative humidity below 60 percent yields the highest number of predicted bikes
 \item When humidity is above 60 percent, then the higher the humidity, the less bikes are predicted to be rented
 \item Not many days had a humidity below 40 percent, so interpret with care for this range
\end{itemize}

\pagebreak

<<pdp-9, echo=FALSE, out.width = "70%", message = FALSE, warning = FALSE>>=
set.seed(123)
pred.bike = Predictor$new(mod, data = bike)
pdp.2feature = FeatureEffect$new(pred.bike, feature = c("temp", "hum"), method = "pdp")
pdp.2feature$plot() + scale_x_continuous('Temperature', limits = c(0, NA)) +  scale_y_continuous('Humidity', limits = c(0, NA))
@

\begin{itemize}
 \item Humidity and temperature interact with each other
 \item Number of bike rentals is especially high when humidity is below 75 percent and temperature is between 15 $^{\circ}$C and 27 $^{\circ}$C
\end{itemize}
\end{vbframe}


\begin{vbframe}{Partial Dependence Plots}
\begin{itemize}
 \item Interpretation: The curve represents the average model prediction when we force all data points to take on a certain (grid) value
 \item The averaging might obfuscate heterogeneous effects and interactions
$\Rightarrow$ Ideally plot ICE and PDP curves together
\lz
 \item For each average prediction, all observations are used, ignoring whether the combination of the grid value and a particular observation results in a likely data point\\
$\Rightarrow$ Can result in unrealistic combination of feature values

\lz
\end{itemize}
\end{vbframe}

\begin{vbframe}{Extrapolation with Correlated Features}

There are two sources of extrapolation when estimating the PD with correlated features:

\begin{itemize}
\item First, the trained model predicts in regions where it was not trained. Model predictions in such regions are a bad approximation to the real underlying functional relationship between the input space and the target space.
\item  Second, the Monte Carlo integral corresponds to an integration with respect to a uniform distribution. It might be better to integrate w.r.t. the (empirical) data distribution.
\end{itemize}

As a result, PD estimates can be biased when features are correlated.


\end{vbframe}

\begin{vbframe}{Extrapolation with Correlated Features}

<<out.width = "70%">>=
set.seed(10)
x1 <- runif(20, -5, 5)
x2 <- x1 + rnorm(20, 0, 1)
df_observed <- data.frame(x1, x2)
df_permuted <- data.frame(expand.grid(x1, x2))
names(df_permuted) <- c("x1", "x2")

p1 <- ggplot() +
  geom_point(data = df_observed, aes(x1, x2),
             shape = 3, color = "red", size = 2, stroke = 2) +
  theme_bw()

p2 <- ggplot() +
  geom_point(data = df_permuted, aes(x1, x2),
             shape = 3, color = "green", size = 2, stroke = 1) +
  geom_point(data = df_observed, aes(x1, x2),
             shape = 3, color = "red", size = 2, stroke = 2) +
  theme_bw()

grid.arrange(p1, p2, ncol = 2, respect = TRUE)
@

The features $x_1$ and $x_2$ are strongly correlated. Observed points are colored in red. New combinations of feature values for ICE curves are colored in green.

\end{vbframe}

\begin{vbframe}{Accumulated Local Effects}

\begin{itemize}
\item Accumulated Local Effects (ALE) are a novel method developed by Apley (2016)
\item ALEs are an alternative to the PD that does not suffer from extrapolation when features are correlated
\item The goal of ALEs is to estimate feature effects of various orders for a given predictive model, i.e., first order effects (main effects of a single feature), second order effects (interactions between two features) etc.
\item Idea: Take the partial derivative of the prediction function with respect to a feature and integrate it with respect to the same feature. This removes unwanted effects of other features

\end{itemize}

{\tiny{Apley, Daniel W. "Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models " ArXiv e-prints (Dec 2016)}}

\end{vbframe}


\begin{vbframe}{Derivative and Subsequent Integral}

\begin{itemize}

\item Consider an additive prediction function $f(x_1, x_2) = 2x_1 + 2x_2 - 4x_1 x_2$
\lz
\item Partial derivative of $f$ with respect to $x_1$: $\frac{\partial f(x_1, x_2)}{\partial x_1} = 2 - 4x_2$
\lz
\item Integral of the partial derivative:  $\int_{z_0}^{x} \frac{\partial f(x_1, x_2)}{\partial x_1} dx_1 = \left[2x_1 - 4x_1 x_2\right]_{z_0}^{x}$
\lz
\item We removed the main effect of $x_2$ which was our goal

\end{itemize}

\end{vbframe}


\begin{vbframe}{First Order ALE}

Let $z_0$ denote the minimum of observed values of feature $x_j$, i.e., $z_0 = min(\mathbf{x_j})$. All complementary features are denoted by $\mathbf{x_{\setminus j}}$.The uncentered first order ALE $\tilde{f}_{j, ALE}(x)$ at point $x$ is defined as:
$$
\begin{aligned}
\tilde{f}_{j, ALE}(x) &= \int_{z_{0}}^{x} \mathbb{E}_{\mathbf{x_{\setminus j}} \vert x_j} \left[\frac{\partial f(x_j, \mathbf{x_{\setminus j}})}{\partial x_j} \bigg \vert x_j = z_j \right] dz_j \\
&= \int_{z_{0}}^{x} \left[ \int \frac{\partial f(z_j, \mathbf{x_{\setminus j}})}{\partial z_j} d\mathcal{P}(\mathbf{x_{\setminus j}} | z_j) \,   \right] dz_j
\end{aligned}
$$

A constant, i.e., the average of the uncentered ALE curve is substracted such that the centered ALE curve $f_{j, ALE}(x)$ has a mean of zero with respect to the marginal distribution of $x_j$:

$$
\begin{aligned}
f_{j, ALE}(x) = \tilde{f}_{j, ALE}(x) - \int_{-\infty}^{\infty}\tilde{f}_{j, ALE}(x_j) \, d\mathcal{P}(x_j)
\end{aligned}
$$

\end{vbframe}

\begin{vbframe}{ALE Estimation}

\begin{itemize}
\item In order to estimate the first order ALE we have to approximate the partial derivative of the prediction function. We do so by computing finite differences of predictions within a set of $K$ intervals for the value range of observed feature values $\mathbf{x_j}$:

$$
\begin{aligned}
x \in [min(\mathbf{x_j}), max(\mathbf{x_j})] \iff &x \in [z_{0, j}, z_{1, j}] \\
\lor &x \in \; ]z_{1, j}, z_{2, j}] \\
&\dots \\
\lor &x \in \; ]z_{K-1, j}, z_{K, j}]
\end{aligned}
$$
\item A simple way of creating $K$ intervals for the value range of $\mathbf{x_j}$ is to use the values of a quantile distribution with $K-1$ quantiles as interval boundaries (not counting the 0\% and 100\% quantiles)

\item We compute a finite difference of predictions for each observation. Consider the $i$-th observation $\mathbf{x^{(i)}} = (x_j^{(i)}, \mathbf{x_{\setminus j}^{(i)}})$. The observation's feature value $x_j^{(i)}$ is located within the $k$-th interval of $\mathbf{x_j}$, i.e., $x_j^{(i)} \in \; ]z_{k-1, j}, z_{k, j}]$. We substitute $x_j^{(i)}$ by the right and left interval boundaries while $\mathbf{x_{\setminus j}^{(i)}}$ is kept constant. The finite difference corresponds to $f(z_{k, j}, \mathbf{x_{\setminus j}^{(i)}}) - f(z_{k-1, j}, \mathbf{x_{\setminus j}^{(i)}})$
\item For each interval, we estimate the local effect by averaging all observation-wise finite differences. This replaces the inner integral, i.e., the integral with respect to the conditional distribution of $x_{\setminus j}$ on $x_j$
\item Summing up the local effects of all intervals up to the point of interest replaces the outer integral
\end{itemize}

\end{vbframe}

\begin{vbframe}{ALE Estimation: Illustration}

Consider the following data generating process:

$$
\begin{gathered}
u_1, u_2 =  \{-10, -9.9, -9.8, \dots, 9.8, 9.9, 10\} \\
n_1, n_2 \stackrel{iid}{\sim}  N(0, 1) \\
\varepsilon \stackrel{iid}{\sim} N(0, 3) \\
\\
x_1 = u_1 + n_1 \\
x_2 = u_2 + n_2 \\
y = 100  \left[ \frac{\partial^2 \left[ \frac{1}{1 + exp(-x_1)} \right] }{\partial x_1 \partial x_1} \right] + 2 x_2 + \varepsilon
\end{gathered}
$$

\newpage

<<data-3d-intervals, out.width = "80%">>=
require("plot3D")
fn_sig = function(x) 1 / (1 + exp(-x))
fn_sigd = function(x) (1 + exp(-x))^(-2) * exp(-x)
fn_sig2d = function(x) ((1 + exp(-x))^(-2) * exp(-x) * (-1)) + (exp(-x)^2 * (2) * (1 + exp(-x))^(-3))
fn_sig3d = function(x) (6 * (1 + exp(-x))^(-4) * exp(-x) * exp(-x)^2 - 2 * (1 + exp(-x))^(-3) * 2 * exp(-x)^2) + (-2 * (1 + exp(-x))^(-3) * exp(-x)^2 + (1 + exp(-x))^(-2) * exp(-x))


set.seed(500)
x1 = seq(-10, 10, 0.1)
x1 = x1 + rnorm(length(x1), 0, 1)
x2 = seq(-10, 10, 0.1)
x2 = x2 + rnorm(length(x2), 0, 1)
errors = rnorm(length(x1), 0, 3)
y = 100 * fn_sig2d(x1) + 2 * x2 + errors
df = data.frame(x1, x2, y)
pred_fun = function(X.model, newdata) as.numeric(predict(X.model, newdata))
tsk = makeRegrTask(target = "y", data = df)
mod = train("regr.ksvm", tsk)

predicted = predict(mod, newdata = df)
prediction = data.frame(x1, x2, y = predicted$data$response)

interval.boundaries = quantile(x1, prob = seq(0, 1, 0.1), type = 1)
# length(interval.boundaries)
interval.boundaries.left = interval.boundaries[-length(interval.boundaries)]
interval.boundaries.right = interval.boundaries[-1]

# plot(x, y)
# plot(x, fn_sig3d(x))
x1lim = range(x1)
x2lim = range(x2)
ylim = range(y)
grid.lines = 25
x1.pred <- seq(min(x1), max(x1), length.out = grid.lines)
x2.pred <- seq(min(x2), max(x2), length.out = grid.lines)
x1x2 <- expand.grid(x1 = x1.pred, x2 = x2.pred)
y.pred <- matrix(predict(mod, newdata = x1x2)$data$response,
                 nrow = grid.lines, ncol = grid.lines)
# fitted points for droplines to surface

par(mar = c(1, 1, 1, 2))

colfunc <- colorRampPalette(c("red", "yellow", "springgreen", "royalblue"))
p = scatter3D(x1, x2, y, ticktype = "detailed",
              bg = "black", pch = 21, lwd = 2, cex = 2.5, col = colfunc(20),
              surf = list(x = x1.pred, y = x2.pred, z = y.pred,
              facets = NA, fit = prediction$y, col = "steelblue"),
              xlim = x1lim, ylim = x2lim, zlim = ylim,
              xlab = "x1", ylab = "x2", zlab = "y",
              theta = 40, phi = 00,
              bty = "b2",
              xaxs = "i", yaxs = "i")

addHyperplane = function(x.value) {

  x2.min = x2lim[1]
  x2.max = x2lim[2]
  horizontal.line = data.frame(x1 = c(x.value, x.value), x2 = c(x2.min, x2.max), y = c(max(y), max(y)))
  lines(trans3D(x = horizontal.line$x1, y = horizontal.line$x2, z = horizontal.line$y, p),
        col = '#FF0000', lwd = 2.5)
  horizontal.line = data.frame(x1 = c(x.value, x.value), x2 = c(x2.min, x2.max), y = c(min(y), min(y)))
  lines(trans3D(x = horizontal.line$x1, y = horizontal.line$x2, z = horizontal.line$y, p),
        col = '#FF0000', lwd = 2.5)
  vertical.line = data.frame(x1 = c(x.value, x.value), x2 = c(x2.min, x2.min), y = c(max(y), min(y)))
  lines(trans3D(x = vertical.line$x1, y = vertical.line$x2, z = vertical.line$y, p),
        col = '#FF0000', lwd = 2.5)
  vertical.line = data.frame(x1 = c(x.value, x.value), x2 = c(x2.max, x2.max), y = c(max(y), min(y)))
  lines(trans3D(x = vertical.line$x1, y = vertical.line$x2, z = vertical.line$y, p),
        col = '#FF0000', lwd = 2.5)
  # for (i in seq(-10, 10, 0.15)) {
  #   line.3d = trans3D(x = rep(x.value, length(x)), y = y, z = rep(i, length(z)), p)
  #   lines(line.3d, col = '#FF000005')
  # }
}
# addCutlines = function(x.value) {
#   cutline.df = data.frame(x = x.value, y = seq(-11.5, 12, 0.5))
#   cutline.df$z = predict(fit, cutline.df)
#   lines(trans3D(x = cutline.df$x, y = cutline.df$y, z = cutline.df$z, p),
#       col = 'brown', lwd = 2, add = TRUE)
# }

for (i in interval.boundaries) {
  addHyperplane(i)
  # addCutlines(i)
}

@

3D visualization of the data, the predictions made by the predictive model (blue net) and 10 intervals for the value range of $x_1$.

\newpage

<<ale-interval-computations>>=
df.filtered = df[df$x1 <= interval.boundaries[9] & df$x1 > interval.boundaries[8], ]
df.filtered.right = df.filtered
df.filtered.right$x1 = interval.boundaries[9]
df.filtered.left = df.filtered
df.filtered.left$x1 = interval.boundaries[8]
df.filtered.permuted = rbind(df.filtered.right, df.filtered.left)

predictions.right = predict(mod, newdata = df.filtered.right)$data$response
df.filtered.right$pred = predictions.right
predictions.left = predict(mod, newdata = df.filtered.left)$data$response
df.filtered.left$pred = predictions.left

addHyperplaneSubset = function(x.value) {
  y = df.filtered$y
  x2 = df.filtered$x2
  horizontal.line = data.frame(x1 = c(x.value, x.value), x2 = c(min(x2), max(x2)), y = c(max(y), max(y)))
  lines(trans3D(x = horizontal.line$x1, y = horizontal.line$x2, z = horizontal.line$y, p),
        col = '#FF0000', lwd = 3.5)
  horizontal.line = data.frame(x1 = c(x.value, x.value), x2 = c(min(x2), max(x2)), y = c(min(y), min(y)))
  lines(trans3D(x = horizontal.line$x1, y = horizontal.line$x2, z = horizontal.line$y, p),
        col = '#FF0000', lwd = 3.5)
  vertical.line = data.frame(x1 = c(x.value, x.value), x2 = c(min(x2), min(x2)), y = c(max(y), min(y)))
  lines(trans3D(vertical.line$x1, vertical.line$x2, vertical.line$y, p),
        col = '#FF0000', lwd = 3.5)
  vertical.line = data.frame(x1 = c(x.value, x.value), x2 = c(max(x2), max(x2)), y = c(max(y), min(y)))
  lines(trans3D(vertical.line$x1, vertical.line$x2, vertical.line$y, p),
        col = '#FF0000', lwd = 3.5)
  # for (i in seq(-10, 10, 0.15)) {
  #   line.3d = trans3D(x = rep(x.value, length(x)), y = y, z = rep(i, length(z)), p)
  #   lines(line.3d, col = '#FF000005')
  # }
}
@


<<ale-interval-zoom, out.width = "80%">>=
colfunc <- colorRampPalette(c("springgreen", "royalblue"))
# par("mar" = c(4, 1, 1, 1))
p = scatter3D(df.filtered$x1,
              df.filtered$x2,
              df.filtered$y,
              ticktype = "detailed",
              bg = "black", pch = 21, lwd = 3, cex = 4, col =  colfunc(10),
              # surf = list(x = x.pred, y = y.pred, z = z.pred,
              # facets = NA, fit = fitpoints, col = "steelblue"),
              theta = 40, phi = 00,
              xlab = "x1", ylab = "x2", zlab = "y",
              # xlim = c(interval.boundaries[8], interval.boundaries[9]),
              # zlim = c(min(df.filtered$y), max(df.filtered$y)),
              bty = "b2",
              xaxs = "i", yaxs = "i")
addHyperplaneSubset(interval.boundaries[8] + 0.05)
points(trans3D(df.filtered$x1, df.filtered$x2, df.filtered$y, p),
        bg = "black", pch = 21, lwd = 3, cex = 4, col =  colfunc(10))
addHyperplaneSubset(interval.boundaries[9])
@

Zooming in on the interval $x_1 \in [\Sexpr{interval.boundaries[8]}, \Sexpr{interval.boundaries[9]}]$.

\newpage


<<ale-interval-substitute, out.width = "80%">>=
colfunc <- colorRampPalette(c("springgreen", "royalblue"))
# par("mar" = c(4, 1, 1, 1))
p = scatter3D(x = df.filtered.permuted$x1,
              y = df.filtered.permuted$x2,
              z = df.filtered.permuted$y,
              ticktype = "detailed",
              bg = "black", pch = 21, lwd = 3, cex = 4, col =  colfunc(10),
              # surf = list(x = x.pred, y = y.pred, z = z.pred,
              # facets = NA, fit = fitpoints, col = "steelblue"),
              theta = 00, phi = 00,
              xlab = "x1", ylab = "x2", zlab = "y",
              # xlim = c(interval.boundaries[8] - 0.5, interval.boundaries[9] + 0.5),
              # zlim = c(min(df.filtered$y) - 1, max(df.filtered$y) + 1),
              bty = "b2",
              xaxs = "i", yaxs = "i")

addHyperplaneSubset(interval.boundaries[8])
addHyperplaneSubset(interval.boundaries[9])
# points(trans3D(df.filtered.left$x, df.filtered.left$y, df.filtered.left$z, p), pch = 21, bg = "black", col = "green")
# points(trans3D(df.filtered.right$x, df.filtered.right$y, df.filtered.right$z, p), pch = 21, bg = "black", col = "green")

points(trans3D(df.filtered.permuted$x1,
               df.filtered.permuted$x2,
               df.filtered.permuted$y, p),
        bg = "black", pch = 21, lwd = 3, cex = 4, col =  colfunc(10))
# for (i in 1:nrow(df.filtered)) {
#   point.right = df.filtered.right[i, ]
#   point.left = df.filtered.left[i, ]
#   line.coords = rbind(point.left, point.right)
#   line.coords.trans= trans3D(
#     x = line.coords$x1,
#     y = line.coords$x2,
#     z = line.coords$y,
#     p)
#   lines(line.coords.trans, col = "black", lwd = 6)
#   lines(line.coords.trans, col = "#E69F00", lwd = 3)
# }
@

We substitute each observation`s $x_1$ value by the right and left interval boundaries, predict and take the difference.

\newpage

<<ale-interval-finitediff, out.width = "80%">>=
par("mar" = c(4, 1, 1, 1))
p = scatter3D(x = df.filtered.permuted$x1,
              y = df.filtered.permuted$x2,
              z = df.filtered.permuted$y,
              ticktype = "detailed", col = rgb(0, 0, 0, max = 255, alpha = 0),
              # surf = list(x = x.pred, y = y.pred, z = z.pred,
              # facets = NA, fit = fitpoints, col = "steelblue"),
              theta = 40, phi = 00,
              xlab = "x1", ylab = "x2", zlab = "y",
              # xlim = c(interval.boundaries[8] - 0.5, interval.boundaries[9] + 0.5),
              # zlim = c(min(df.filtered$y) - 1, max(df.filtered$y) + 1),
              bty = "b2",
              xaxs = "i", yaxs = "i")

addHyperplaneSubset(interval.boundaries[8])

for (i in 1:nrow(df.filtered)) {
  y.right = predictions.right[i]
  y.left = predictions.left[i]
  x1.left = interval.boundaries[8]
  x1.right = interval.boundaries[9]
  x2.fixed = df.filtered$x2[i]
  point.left = data.frame("x1" = x1.left, "x2" = x2.fixed, "y" = y.left)
  point.right = data.frame("x1" = x1.right, "x2" = x2.fixed, "y" = y.right)
  line.coords = rbind(point.left, point.right)
  line.coords.trans = trans3D(
    x = line.coords$x1,
    y = line.coords$x2,
    z = line.coords$y,
    p)
  lines(line.coords.trans, col = "black", lwd = 8)
  lines(line.coords.trans, col = "#E69F00", lwd = 4)
}
addHyperplaneSubset(interval.boundaries[9])
@

For each observation, we receive a change in prediction when traversing the interval from the left to the right boundary.

\newpage

<<ale-interval-average, out.width = "80%">>=
par("mar" = c(4, 1, 1, 1))
p = scatter3D(x = df.filtered.permuted$x1,
              y = df.filtered.permuted$x2,
              z = df.filtered.permuted$y,
              ticktype = "detailed", col = rgb(0, 0, 0, max = 255, alpha = 0),
              # surf = list(x = x.pred, y = y.pred, z = z.pred,
              # facets = NA, fit = fitpoints, col = "steelblue"),
              theta = 00, phi = 00,
              xlab = "x1", ylab = "x2", zlab = "y",
              # xlim = c(interval.boundaries[8] - 0.5, interval.boundaries[9] + 0.5),
              # zlim = c(min(df.filtered$y) - 1, max(df.filtered$y) + 1),
              bty = "b2",
              xaxs = "i", yaxs = "i")
addHyperplaneSubset(interval.boundaries[8])
addHyperplaneSubset(interval.boundaries[9])
for (i in 1:nrow(df.filtered)) {
  y.right = mean(predictions.right)
  y.left = mean(predictions.left)
  x1.left = interval.boundaries[8]
  x1.right = interval.boundaries[9]
  x2.mean = mean(df.filtered$x2)
  point.left = data.frame("x1" = x1.left, "x2" = x2.mean, "y" = y.left)
  point.right = data.frame("x1" = x1.right, "x2" = x2.mean, "y" = y.right)
  line.coords = rbind(point.left, point.right)
  line.coords.trans = trans3D(
    x = line.coords$x1,
    y = line.coords$x2,
    z = line.coords$y,
    p)
  lines(line.coords.trans, col = "black", lwd = 8)
  lines(line.coords.trans, col = "#E69F00", lwd = 4)
}
@

We integrate over the conditional distribution of $x_2$ on $x_1$ by averaging all observation-wise finite differences inside each interval.

\newpage

<<3d-ale-final, out.width = "70%">>=
iml_pred = Predictor$new(mod, df)
ale = FeatureEffect$new(iml_pred, "x1", method = "ale", grid.size = 10)
p = ale$plot() +
  geom_line(size = 3, color = "black") +
  geom_line(size = 1.5, color = "#E69F00") +
  geom_vline(xintercept = interval.boundaries) +
  geom_vline(xintercept = c(interval.boundaries[8], interval.boundaries[9]),
             color = "red", size = 1.5) +
  scale_y_continuous('First order ALE of x1')
plot(p)
@

We repeat the same procedure for every interval. The first order ALE corresponds to adding up all average interval-wise finite differences and substracting the centering constant.

\end{vbframe}

\begin{vbframe}{ALE Estimation: Formula}

Let $k_j(x)$ denote the interval index a feature value $x \in \mathbf{x_j}$ falls in. $n_j(k)$ denotes the number of observations inside the $k$-th interval of $\mathbf{x_j}$. The estimated uncentered first order ALE $\hat{\tilde{f}}_{j, ALE}(x)$ at point $x$ corresponds to:
$$
\begin{aligned}
\hat{\tilde{f}}_{j, ALE}(x) = \sum_{k = 1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i: \; x_j^{(i)} \in \; ]z_{k-1, j}, z_{k, j}]}\left[f(z_{k, j}, \mathbf{x^{(i)}_{\setminus{}j}}) - f(z_{k-1, j}, \mathbf{x^{(i)}_{\setminus{}j}})\right]
\end{aligned}
$$

In order to receive the centered ALE estimate $\hat{f}_{j, ALE}(x)$ at point $x$, we substract the average of the estimated uncentered ALE curve:

$$
\begin{aligned}
\hat{f}_{j, ALE}(x) = \hat{\tilde{f}}_{j, ALE}(x) - \frac{1}{n}\sum_{i = 1}^n \hat{\tilde{f}}_{j, ALE}(x_j^{(i)})
\end{aligned}
$$

\end{vbframe}

\begin{vbframe}{ALE Estimation: Algorithm}


\begin{itemize}
	\item Create $K$ intervals for the value range of $\mathbf{x_j}$
	\item For each interval $k \in 1, \dots, k_j(x)$:
	  \begin{itemize}
	  \item Select subset of observations inside the $k$-th interval
	  \item For each observation inside $k$-th interval: compute observation-wise finite difference
	  \item Average all observation-wise finite differences inside $k$-th interval to interval-wise local effect
	  \end{itemize}
  \item Sum up all interval-wise local effects up to point of interest $x$ to uncentered ALE estimate
  \item Center the uncentered ALE estimate
\end{itemize}

\end{vbframe}


\begin{vbframe}{Example: First Order ALE}

The PD (left) often looks very similar to the first order ALE (right) but on a different scale. In the case of correlated features, the ALE is a better option due to the PD's extrapolation issues.

<<example-ALE-first-order, out.width = "70%">>=
set.seed(123)
pdp = FeatureEffect$new(pred.bike, "hum", method = "pdp", grid.size = 50)
pdp_plot <- pdp$plot() + scale_y_continuous('Univariate PD on humidity')
ale = FeatureEffect$new(pred.bike, "hum", method = "ale", grid.size = 50)
ale_plot <- ale$plot() + scale_y_continuous('First order ALE of humidity')
grid.arrange(pdp_plot, ale_plot, nrow = 1, ncol = 2)
@

\end{vbframe}


\begin{vbframe}{Example: Second Order ALE}

It is possible to estimate ALEs of higher order, e.g., second order ALEs. As opposed to the bivariate PD, the second order ALE corresponds to an estimate of the interaction between two features only, i.e., first order effects have been substracted.

\vspace{0.1cm}

<<example-ALE-second-order, out.width = "70%">>=
set.seed(123)
pdp = FeatureEffect$new(pred.bike, feature = c("temp", "hum"), method = "pdp", grid.size = 10)
pdp_plot <- pdp$plot() + theme(legend.position = "top", legend.key.width = unit(1.5,"cm")) + ggtitle("Bivariate PD")
ale = FeatureEffect$new(pred.bike, feature = c("temp", "hum"), method = "ale", grid.size = 10)
ale_plot <- ale$plot() + theme(legend.position = "top", legend.key.width = unit(1.5,"cm")) + ggtitle("Second order ALE")
grid.arrange(pdp_plot, ale_plot, nrow = 1, ncol = 2)

#
# set.seed(123)
# pred.bike = Predictor$new(mod, data = bike)
# pdp.2feature = FeatureEffect$new(pred.bike, feature = c("temp", "hum"), method = "pdp")
# pdp.2feature$plot() + scale_x_continuous('Temperature', limits = c(0, NA)) +  scale_y_continuous('Humidity', limits = c(0, NA))
@

\end{vbframe}

\begin{vbframe}{Extrapolation and the ALE}
\begin{itemize}

\item Remember that there are two sources of extrapolation when estimating the PD with correlated features: The model predicts in regions where it was not trained, and the Monte Carlo integral corresponds to an integration w.r.t. a uniform distribution, instead of the data distribution
\item Assuming that all interval boundaries are sufficiently close to the corresponding observed values, ALEs are robust to both sources of extrapolation. First, the trained model does not predict in regions where it was not trained. Second, we integrate with respect to the conditional distribution of $x_{\setminus j}$ on $x_j$ instead of the marginal distribution of $x_{\setminus j}$, by only averaging the finite differences inside a single interval
\end{itemize}
\end{vbframe}

%
% \begin{vbframe}{Additive Unbiasedness of the ALE}
%
%
% \begin{itemize}
% \item By taking the finite difference of predictions where only $\mathbf{x_j}$ changes values and $\mathbf{x_{\setminus j}}$ is kept constant, additively linked effects of other features are removed
% \item This important property of ALEs is called additive unbiasedness
% \end{itemize}
% Consider an additive prediction function $f: \R^p \mapsto \R$ where $x_j$ only contributes with a main effect, i.e., there are no interactions with other features. The finite difference on the interval $[z_{k-1}, z_k]$ corresponds to:
% \begin{align*}
%  &\phantom{{}={}} f(z_k, \mathbf{x_{\setminus j}}) - f(z_{k-1}, \mathbf{x_{\setminus j}}) \nonumber \\
% &= (f_0 + f_j(z_k) + \sum\limits_{l = 1}^p f_{l}(\mathbf{x_l}) + \sum\limits_{l \neq h} f_{lh}(\mathbf{x_l}, \mathbf{x_h}) + \dots ) \nonumber \\
% &\phantom{{}={}} - (f_0 + f_j(z_{k-1}) + \sum\limits_{l = 1}^p f_{l}(\mathbf{x_l}) + \sum\limits_{l \neq h} f_{lh}(\mathbf{x_l}, \mathbf{x_h}) + \dots ) \nonumber \\
% &= f_j(z_k) - f_j(z_{k-1})
% \end{align*}
% \end{vbframe}


% Functional Decomposition and Functional Anova

\begin{vbframe}{Functional Decomposition}
Any prediction function $f: \R^p \mapsto \R$ can be decomposed into components of increasing order.
\begin{eqnarray*}\label{eqn:decomp} f(x)  =& \overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j<k}^p f_{jk}(x_j, x_k)}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_1, \ldots, x_p)}^\text{p-th order effect} \\
	=&  \sum_{S\subseteq D} f_S(x_S)\\
%\\ =  & \sum_{S \subseteq \{1,\ldots,p\}} f_S(x_S)
\end{eqnarray*}
	where $D = \{1, \ldots, p\}$


\pagebreak
\begin{itemize}
\item The decomposition is only unique with additional constraints for the components
\item The decomposition can be used to
\begin{itemize}
	\item extract main effects
	\item find lower dimensional approximations of the original model
	\item analyze how each component contributes to the variance of the prediction function (functional ANOVA)
\end{itemize}
\end{itemize}
\end{vbframe}


\begin{vbframe}{Functional Anova}
Hooker (2004) proposed following decomposition:


$$f_S(x) = \int_{X_{-S}}\left(F(x) - \sum_{v\subset S} f_v(x)\right) dx_{-S}$$

It can be shown that the $f_u(x)$ fulfill following properties:
\begin{itemize}
\item Zero Means: $\int f_S (x_S)d x_S = 0$ for each $S\neq \emptyset$
\item Orthogonality: $\int f_S(x_S) f_V(x_V)dx = 0$ for $S\neq V$
\item Variance Decomposition: Let $\sigma^2_{(f_S)} = \int f_S(x)^2dx$ then $\sigma^2(f) = \sum_{S\subseteq D} \sigma^2_S(f_S)$
\end{itemize}
\tiny{Giles Hooker, 2004. "Discovering ANOVA Structure in Black Box Functions". Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. }

\end{vbframe}

\begin{vbframe}{Functional Anova}
	\begin{itemize}
		\item Components can be estimated similar to partial dependence plots
		\item $f_0$ (the intercept) is estimated as the overall mean: $f_{0} = \frac{1}{n}\sum_{i=1}^n \fh(\xi)$
		\item Requires independence between the features
		\item For dependent features, Hooker (2007) suggested a different approach which requires knowledge of the data distribution of x and is computationally expensive
	\end{itemize}
\tiny{Hooker, Giles. "Generalized functional anova diagnostics for high-dimensional functions of dependent variables." Journal of Computational and Graphical Statistics 16.3 (2007): 709-732.}


\end{vbframe}



%%lime
\begin{vbframe}{LIME}
Local Interpretable model-agnostic Explanations
\begin{itemize}
        \item  Fits local, interpretable models that can explain individual predictions of any black-box model
        \item  Local surrogate models, that are interpretable like a LM or CART and are learned on predictions of original model
\end{itemize}
\begin{center}
 \includegraphics{figure_man/lime1}
\end{center}
\vspace{.5cm}
\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier}

\pagebreak
\normalsize

How to fit a local surrogate model\\
1. Choose instance of interest x\\
2. Perturb data and get black box predictions for them\\
3. Weight new samples by their proximity to x\\
4. Fit a weighted, sparse, interpretable model on this new data set (e.g. LASSO)\\
\begin{center}
 \includegraphics[width=0.52\textwidth]{figure_man/lime}
\end{center}
\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier}

\pagebreak
\normalsize

Example for 2 instances.
The plots show the effect of the sparse linear model (model coefficients times feature value of instance).
The total prediction is roughly the sum of the effects.
\vspace{0.25cm}

<<lime-1, echo = FALSE, out.width = "70%", message = FALSE>>=

library(gower)
set.seed(123)
instance_indices = c(295, 8)
n_features_lime = 3
mod = train("regr.randomForest", task)
pred = Predictor$new(mod, data = bike.x, class = "above")
lim = LocalModel$new(pred, x.interest = bike.x[instance_indices[1],], k = n_features_lime)
a = plot(lim)

lim = LocalModel$new(pred, x.interest = bike.x[instance_indices[2],], k = n_features_lime)
b = plot(lim)

grid.arrange(a, b, nrow = 2, ncol = 1)
@

\pagebreak

\begin{itemize}
 \item Any interpretable model (LASSO, decision tree, ...) can be used as local model
\lz
 \item Assumes that even if a machine learning model is very complex, the local prediction can be described with a simpler model
\lz
 \item Difficult: Defining locality (= how samples are weighted locally). Has a huge influence on the local model, but there is no automatic procedure for choosing the neighbourhood
\lz
 \item The LIME method can also be used for image or text classifiers
\end{itemize}
\end{vbframe}


\begin{vbframe}{LIME for Images and Text}
LIME for images and text differs from LIME for tabular data:
\lz
\begin{itemize}
 \item For text classification is a binary vector indicating the presence
or absence of a word
\lz
 \item For images classification is a binary vector indicating the presence or absence of a contiguous patch of similar pixels (also "super-pixel" ), since a pixel would probably not change the predictions by much
\end{itemize}
\vspace{3cm}
\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier}

\pagebreak
\normalsize

\begin{center}
 \includegraphics[width=0.8\textwidth]{figure_man/lime-images}
\end{center}
\lz
\begin{center}
 \includegraphics[width=0.8\textwidth]{figure_man/lime-text}
\end{center}
\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier}
\tiny{Marco Tulio Correia Ribeiro. 2016. Retrieved from https://github.com/marcotcr/lime}
\end{vbframe}


\begin{vbframe}{Possible Problems of LIME}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
 \item For LIME with tabular data (e.g. bike data), the correct definition of the neighbourhood is a very big, unsolved problem.
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}  %%<--- here
\begin{center}
 \includegraphics[width=0.72\textwidth]{figure_man/lime-fail-1}
\end{center}
\end{column}
\end{columns}
\lz
\begin{itemize}
 \item You have to try out different kernel settings and see if it makes sense. (LIME currently uses an exponential smoothing kernel to define the neighbourhood)
\lz
 \item The instability of the explanations is a really big issue. Instability likely implies that explanations should be taken with a grain of salt.
\end{itemize}
\end{vbframe}



%%Shapley Values

\begin{vbframe}{Shapley Values in Game Theory}
\begin{itemize}
\item Game theory is a mathematical field concerned with modeling strategic interactions between rational decision makers
\lz
\item In coalitional game theory, a set of $p$ players play games and join coalitions. They are rewarded with a payout. The characteristic function $v: 2^p \mapsto \mathcal{R}$ maps all player coalitions to their respective payouts
\lz
\item The Shapley value is a player‚Äôs average contribution to the payout, i.e., the marginal increase in payout for the coalition of players, averaged over all possible coalitions

\end{itemize}

\end{vbframe}

\begin{vbframe}{Shapley Values for Feature Effects}
\begin{itemize}
\item For Shapley values in machine learning, predicting the target for a single observation corresponds to the game and a coalition of features represents the players
\lz
 \item The Shapley value tells us how to fairly distribute the \enquote{payout} among features
$\to$ Another way to explain individual predictions
\lz
 \item The Shapley Value is the average marginal contribution of a feature value to the prediction over all possible coalitions of feature values
 \lz
 \item Sum of Shapley Values over all features yields the difference between the average prediction of all data points and the selected individual prediction
\end{itemize}

\pagebreak
 \begin{figure}
 \begin{center}
\tikzset{
    vertex/.style = {
        style = every edge,
        fill = white , draw = gray,
        font =\scriptsize,
        text width = 2.6cm,
        text justified, text centered
    }
}
\begin{tikzpicture}[scale = .8]
\node [rectangle,vertex] (a) at (-5, 8) {Players};
\node [rectangle,vertex] (b) at (0, 8) {Game};
\node [rectangle,vertex] (c) at (5, 8) {Payout};
\node [rectangle,vertex] (d) at (-5, 6) {The feature values of the instance, which collaborate to receive the gain};
\node [rectangle,vertex] (e) at (0, 6) {Predict outcome for a data instance};
\node [rectangle,vertex] (f) at (5, 6) {The prediction for this instance minus the average prediction of all instances};
\node [rectangle, draw=white] (o) at (-2.6, 4.2) {An example for the bike rental data set :};
\node [rectangle,vertex] (g) at (-5, 3) {Weather=Misty};
\node [rectangle,vertex] (k) at (-5, 2) {Humidity=90\%};
\node [rectangle,vertex] (l) at (-5, 1) {Season=Fall};
\node [rectangle,vertex] (m) at (-5, 0) {Temperature=5};
\node [rectangle,vertex] (n) at (-5, -1) {...};
\node [rectangle,vertex] (h) at (0, 1) {Predict number of bikes for data instance};
\node [rectangle,vertex] (i) at (5, 1) {Predicted number of bikes for this instance minus the average prediction of all instances};
\draw[->,thick] (a) -- (b);
\draw[->,thick] (b) -- (c);
\draw[->,thick] (d) -- (e);
\draw[->,thick] (e) -- (f);
\draw[->,thick] (a) -- (d);
\draw[->,thick] (b) -- (e);
\draw[->,thick] (c) -- (f);
\draw[->,thick] (h) -- (i);
\draw[->,thick] (-3.2, 3) -- (-1.8, 1.75);
\draw[->,thick] (-3.2, 2) -- (h);
\draw[->,thick] (-3.2, 1) -- (h);
\draw[->,thick] (-3.2, 0) -- (h);
\draw[->,thick] (-3.2, -1) --(-1.8, .25);
\end{tikzpicture}
\end{center}
\end{figure}
\end{vbframe}


\begin{vbframe}{Notation}
\begin{itemize}
 \item $x$ : data
 \item $i$ : an instance of interest
 \item $j$ : a feature
 \item $\hat{f}$ : a machine learning model
 \item $M$ : the number of samples with $m \in \left\{ 1,2,...,M \right\}$
 \item $\phi_{ij}^{(m)}$ : Shapley value, the average marginal contribution of feature value $x_{ij}$
 \item $\hat{f}(x^{*+j})$ : the prediction for $x_i.$, but with a random number of features values replaced by feature values from a random data point $x$, excluding the feature value for $x_{ij}$
 \item $x^{*-j}$ : the x-vector, almost identical to $x^{*+j}$, but the value $x_{ij}$ is also taken from the sampled $x$
\end{itemize}

\pagebreak

\begin{itemize}
 \item Shapley Value approximation with Monte-Carlo sampling\\
 \item Estimation of one Shapley contribution $\phi_{ij}^{(m)}$\\
\begin{center}
  \includegraphics[width=0.8\textwidth]{figure_man/shapley}
\end{center}
 \item Simulation of left out features (players) via sampling from other data points
\end{itemize}

\pagebreak

\begin{itemize}
 \item Compute the Shapley Value as the average: \\
$$ \hat{\phi_{ij}} = \frac{1}{M} \sum_{m=1}^{M}(\hat{f}(x^{*+j}) - \hat{f}(x^{*-j})) $$\\
\lz
 \item Interpretation of Shapley Value $\phi_{ij}$ for feature $j$ and instance $i$ is:The feature value $x_{ij}$ contributed $\phi_{ij}$ towards the prediction for instance $i$ compared to the average prediction for the dataset
\end{itemize}
\vspace{2cm}
\tiny{Shapley, Lloyd S. 1953. "A Value for N-Person Games."}
\tiny{Strumbelj, Erik, Igor Kononenko, Erik Strumbelj, and Igor Kononenko. 2014. "Explaining prediction models and individual predictions with feature contributions."}
\end{vbframe}


\begin{vbframe}{Example Shapley Value}

<<sv-2, echo=FALSE,  message=FALSE, warning=FALSE>>==
set.seed(123)
instance.index = 200
mean.prediction = mean(predictor$predict(bike)[[1]])
instance.prediction = predictor$predict(bike[instance.index,])[[1]]
shap = Shapley$new(predictor, x.interest = bike.x[200,])
strongest.feature = shap$results[shap$results$phi == max(shap$results$phi),]
@

The plot shows the Shapley Value for instance $\Sexpr{instance.index}$. The difference between the instance's model prediction of $\Sexpr{sprintf('%.0f', instance.prediction)}$
and the average prediction of $\Sexpr{sprintf("%.0f", mean.prediction)}$
is fairly distributed among the features. Feature value $\Sexpr{ strongest.feature$feature.value}$ had the largest positive effect, with a contribution (increase in prediction) of +$\Sexpr{ round(strongest.feature$phi)}.$
\vspace{0.5cm}
<<sv-3, echo=FALSE, out.width = "65%", message=FALSE, warning=FALSE>>==
plot(shap)
@
\end{vbframe}


\endlecture
